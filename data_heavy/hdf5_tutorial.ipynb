{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HDF5 and related packages\n",
    "!pip install h5py numpy matplotlib pandas tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 Tutorial - Hierarchical Data Format\n",
    "\n",
    "**HDF5** is the gold standard for scientific data storage:\n",
    "- **Hierarchical**: Groups organize data like filesystem directories\n",
    "- **Self-describing**: Rich metadata and attributes\n",
    "- **Cross-platform**: Works everywhere, language-agnostic\n",
    "- **Efficient**: Chunking, compression, parallel I/O\n",
    "\n",
    "Perfect for: experimental data, simulations, imaging, time series archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"HDF5 version: {h5py.version.hdf5_version}\")\n",
    "print(f\"h5py version: {h5py.version.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Basic HDF5 Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample experimental data\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate multi-instrument experiment\n",
    "time_points = np.linspace(0, 100, 1000)\n",
    "temperature = 25 + 5 * np.sin(time_points * 0.1) + np.random.normal(0, 0.5, 1000)\n",
    "pressure = 1013 + 10 * np.cos(time_points * 0.05) + np.random.normal(0, 2, 1000)\n",
    "spectra = np.random.exponential(1, (1000, 256))  # 1000 spectra, 256 wavelengths\n",
    "wavelengths = np.linspace(400, 800, 256)  # nm\n",
    "\n",
    "# Sample metadata\n",
    "experiment_info = {\n",
    "    'title': 'High-Temperature Spectroscopy Experiment',\n",
    "    'researcher': 'Dr. Smith',\n",
    "    'date': '2024-01-15',\n",
    "    'instrument': 'SpectraMax 3000',\n",
    "    'sample_id': 'HTS-001'\n",
    "}\n",
    "\n",
    "print(f\"Generated experimental data:\")\n",
    "print(f\"  Time series: {len(time_points)} points\")\n",
    "print(f\"  Spectra: {spectra.shape} (time √ó wavelength)\")\n",
    "print(f\"  Total size: {(spectra.nbytes + temperature.nbytes + pressure.nbytes)/1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hierarchical HDF5 structure\n",
    "filename = 'experiment_data.h5'\n",
    "\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    # Root-level metadata\n",
    "    f.attrs['title'] = experiment_info['title']\n",
    "    f.attrs['created'] = datetime.now().isoformat()\n",
    "    f.attrs['hdf5_version'] = h5py.version.hdf5_version\n",
    "    \n",
    "    # Create groups (like directories)\n",
    "    experiment = f.create_group('experiment')\n",
    "    raw_data = f.create_group('raw_data')\n",
    "    processed = f.create_group('processed')\n",
    "    calibration = f.create_group('calibration')\n",
    "    \n",
    "    # Experiment metadata group\n",
    "    for key, value in experiment_info.items():\n",
    "        experiment.attrs[key] = value\n",
    "    \n",
    "    # Raw data with compression and chunking\n",
    "    temp_dset = raw_data.create_dataset('temperature', data=temperature,\n",
    "                                       compression='gzip', compression_opts=9,\n",
    "                                       chunks=True, shuffle=True)\n",
    "    temp_dset.attrs['units'] = '¬∞C'\n",
    "    temp_dset.attrs['instrument'] = 'Thermocouple K-type'\n",
    "    temp_dset.attrs['calibration_date'] = '2023-12-01'\n",
    "    \n",
    "    pres_dset = raw_data.create_dataset('pressure', data=pressure,\n",
    "                                       compression='lzf', chunks=True)\n",
    "    pres_dset.attrs['units'] = 'hPa'\n",
    "    pres_dset.attrs['instrument'] = 'Digital Barometer'\n",
    "    \n",
    "    spec_dset = raw_data.create_dataset('spectra', data=spectra,\n",
    "                                       compression='szip', chunks=(100, 256))\n",
    "    spec_dset.attrs['units'] = 'counts'\n",
    "    spec_dset.attrs['exposure_time'] = '100ms'\n",
    "    \n",
    "    # Coordinate arrays\n",
    "    time_dset = raw_data.create_dataset('time', data=time_points, compression='gzip')\n",
    "    time_dset.attrs['units'] = 'seconds'\n",
    "    \n",
    "    wave_dset = calibration.create_dataset('wavelengths', data=wavelengths)\n",
    "    wave_dset.attrs['units'] = 'nm'\n",
    "    wave_dset.attrs['calibration_polynomial'] = [399.8, 1.56, -0.0001]  # Example\n",
    "    \n",
    "    # Processed data (example: smoothed temperature)\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    temp_smooth = gaussian_filter1d(temperature, sigma=2)\n",
    "    processed.create_dataset('temperature_smoothed', data=temp_smooth,\n",
    "                           compression='gzip')\n",
    "    \n",
    "    # Create links (like symbolic links)\n",
    "    processed['time'] = raw_data['time']  # Hard link to time data\n",
    "\n",
    "print(f\"HDF5 file created: {filename}\")\n",
    "print(f\"File size: {os.path.getsize(filename)/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exploring HDF5 Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hdf5_structure(name, obj, indent=0):\n",
    "    \"\"\"Recursively print HDF5 structure\"\"\"\n",
    "    spaces = '  ' * indent\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        print(f\"{spaces}üìÅ {name}/\")\n",
    "        if obj.attrs:\n",
    "            for attr_name, attr_value in obj.attrs.items():\n",
    "                print(f\"{spaces}   @{attr_name}: {attr_value}\")\n",
    "    elif isinstance(obj, h5py.Dataset):\n",
    "        compression = obj.compression if obj.compression else 'none'\n",
    "        print(f\"{spaces}üìÑ {name}: {obj.shape} {obj.dtype} [{compression}]\")\n",
    "        if obj.attrs:\n",
    "            for attr_name, attr_value in obj.attrs.items():\n",
    "                print(f\"{spaces}   @{attr_name}: {attr_value}\")\n",
    "\n",
    "# Explore the file structure\n",
    "print(\"HDF5 File Structure:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    print(f\"üìÑ {filename}\")\n",
    "    # Print root attributes\n",
    "    for attr_name, attr_value in f.attrs.items():\n",
    "        print(f\"   @{attr_name}: {attr_value}\")\n",
    "    print()\n",
    "    \n",
    "    # Recursively print structure\n",
    "    f.visititems(lambda name, obj: print_hdf5_structure(name, obj, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Reading and Querying HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data efficiently\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    print(\"Dataset Information:\")\n",
    "    \n",
    "    # Access datasets\n",
    "    temp_data = f['raw_data/temperature']\n",
    "    spec_data = f['raw_data/spectra']\n",
    "    \n",
    "    print(f\"Temperature: {temp_data.shape}, compression: {temp_data.compression}\")\n",
    "    print(f\"Spectra: {spec_data.shape}, chunks: {spec_data.chunks}\")\n",
    "    \n",
    "    # Partial reading (efficient for large datasets)\n",
    "    first_100_temps = temp_data[:100]  # Read only first 100 points\n",
    "    spectrum_subset = spec_data[100:200, 50:150]  # Subset of spectra\n",
    "    \n",
    "    print(f\"\\nPartial reads:\")\n",
    "    print(f\"First 100 temperatures: {first_100_temps.shape}\")\n",
    "    print(f\"Spectrum subset: {spectrum_subset.shape}\")\n",
    "    \n",
    "    # Access metadata\n",
    "    experiment_title = f['experiment'].attrs['title']\n",
    "    temp_units = f['raw_data/temperature'].attrs['units']\n",
    "    \n",
    "    print(f\"\\nMetadata:\")\n",
    "    print(f\"Experiment: {experiment_title}\")\n",
    "    print(f\"Temperature units: {temp_units}\")\n",
    "    \n",
    "    # Query capabilities\n",
    "    print(f\"\\nDataset properties:\")\n",
    "    print(f\"Temperature range: {temp_data[:].min():.1f} to {temp_data[:].max():.1f} {temp_units}\")\n",
    "    print(f\"Spectra shape: {spec_data.shape[0]} time points √ó {spec_data.shape[1]} wavelengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hierarchical data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "with h5py.File(filename, 'r') as f:\n",
    "    time = f['raw_data/time'][:]\n",
    "    temp = f['raw_data/temperature'][:]\n",
    "    temp_smooth = f['processed/temperature_smoothed'][:]\n",
    "    pressure = f['raw_data/pressure'][:]\n",
    "    spectra = f['raw_data/spectra'][:]\n",
    "    wavelengths = f['calibration/wavelengths'][:]\n",
    "    \n",
    "    # Temperature time series\n",
    "    axes[0,0].plot(time, temp, 'b-', alpha=0.7, linewidth=0.8, label='Raw')\n",
    "    axes[0,0].plot(time, temp_smooth, 'r-', linewidth=2, label='Smoothed')\n",
    "    axes[0,0].set_xlabel('Time (s)')\n",
    "    axes[0,0].set_ylabel('Temperature (¬∞C)')\n",
    "    axes[0,0].set_title('Temperature vs Time')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pressure time series\n",
    "    axes[0,1].plot(time, pressure, 'g-', linewidth=1)\n",
    "    axes[0,1].set_xlabel('Time (s)')\n",
    "    axes[0,1].set_ylabel('Pressure (hPa)')\n",
    "    axes[0,1].set_title('Pressure vs Time')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Spectral heatmap\n",
    "    im = axes[1,0].imshow(spectra[:200, :].T, aspect='auto', cmap='plasma',\n",
    "                         extent=[time[0], time[199], wavelengths[0], wavelengths[-1]])\n",
    "    axes[1,0].set_xlabel('Time (s)')\n",
    "    axes[1,0].set_ylabel('Wavelength (nm)')\n",
    "    axes[1,0].set_title('Spectral Evolution (first 200 points)')\n",
    "    plt.colorbar(im, ax=axes[1,0], label='Intensity')\n",
    "    \n",
    "    # Average spectrum\n",
    "    avg_spectrum = spectra.mean(axis=0)\n",
    "    axes[1,1].plot(wavelengths, avg_spectrum, 'purple', linewidth=2)\n",
    "    axes[1,1].set_xlabel('Wavelength (nm)')\n",
    "    axes[1,1].set_ylabel('Average Intensity')\n",
    "    axes[1,1].set_title('Time-Averaged Spectrum')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Advanced HDF5 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced features: compound data types, references, external links\n",
    "advanced_file = 'advanced_hdf5.h5'\n",
    "\n",
    "with h5py.File(advanced_file, 'w') as f:\n",
    "    # 1. Compound data types (like struct)\n",
    "    participant_dtype = np.dtype([\n",
    "        ('id', 'i4'),\n",
    "        ('name', 'S20'),\n",
    "        ('age', 'i4'),\n",
    "        ('score', 'f8')\n",
    "    ])\n",
    "    \n",
    "    participants = np.array([\n",
    "        (1, b'Alice', 25, 85.5),\n",
    "        (2, b'Bob', 30, 92.1),\n",
    "        (3, b'Charlie', 28, 78.9)\n",
    "    ], dtype=participant_dtype)\n",
    "    \n",
    "    study_group = f.create_group('study')\n",
    "    study_group.create_dataset('participants', data=participants)\n",
    "    \n",
    "    # 2. Variable-length strings\n",
    "    string_dtype = h5py.special_dtype(vlen=str)\n",
    "    comments = [\"Great experiment!\", \"Need more data points\", \"Interesting results\"]\n",
    "    study_group.create_dataset('comments', data=comments, dtype=string_dtype)\n",
    "    \n",
    "    # 3. Extensible datasets (unlimited dimensions)\n",
    "    unlimited_data = f.create_dataset('streaming_data', \n",
    "                                     shape=(0, 3), maxshape=(None, 3),\n",
    "                                     chunks=True, compression='gzip')\n",
    "    \n",
    "    # Add data in chunks (simulating streaming)\n",
    "    for i in range(5):\n",
    "        new_data = np.random.randn(10, 3)\n",
    "        unlimited_data.resize(unlimited_data.shape[0] + 10, axis=0)\n",
    "        unlimited_data[-10:, :] = new_data\n",
    "    \n",
    "    # 4. References (pointers to other datasets)\n",
    "    refs_group = f.create_group('references')\n",
    "    temp_ref = study_group.ref  # Reference to study group\n",
    "    refs_group.attrs['study_reference'] = temp_ref\n",
    "    \n",
    "    print(f\"Advanced HDF5 features demonstrated:\")\n",
    "    print(f\"  Compound data: {participants.shape} records\")\n",
    "    print(f\"  Variable strings: {len(comments)} comments\")\n",
    "    print(f\"  Extensible dataset: {unlimited_data.shape}\")\n",
    "    print(f\"  References: Created\")\n",
    "\n",
    "print(f\"Advanced file size: {os.path.getsize(advanced_file)/1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading advanced features\n",
    "with h5py.File(advanced_file, 'r') as f:\n",
    "    print(\"Reading Advanced Features:\")\n",
    "    \n",
    "    # Read compound data\n",
    "    participants = f['study/participants'][:]\n",
    "    print(f\"\\nParticipants:\")\n",
    "    for p in participants:\n",
    "        print(f\"  ID: {p['id']}, Name: {p['name'].decode()}, Age: {p['age']}, Score: {p['score']}\")\n",
    "    \n",
    "    # Read variable-length strings\n",
    "    comments = f['study/comments'][:]\n",
    "    print(f\"\\nComments:\")\n",
    "    for i, comment in enumerate(comments):\n",
    "        print(f\"  {i+1}: {comment}\")\n",
    "    \n",
    "    # Show extensible dataset\n",
    "    streaming = f['streaming_data']\n",
    "    print(f\"\\nStreaming data shape: {streaming.shape}\")\n",
    "    print(f\"Last 3 rows:\\n{streaming[-3:, :]}\")\n",
    "    \n",
    "    # Follow reference\n",
    "    study_ref = f['references'].attrs['study_reference']\n",
    "    referenced_group = f[study_ref]\n",
    "    print(f\"\\nReferenced group: {referenced_group.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß HDF5 for compression\n",
    "\n",
    "Here we demonstrate some compression method can be easily used in hdf5.\n",
    "\n",
    "Note that each compression method or combination will really behave differently depending on data type, format, ... so the number are not to be taken as \"`gzip` is the best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Make a compressible array (smooth signal + small noise)\n",
    "nx, ny = 4000, 4000\n",
    "x = np.linspace(0, 50, nx)[:, None]\n",
    "y = np.linspace(0, 50, ny)[None, :]\n",
    "signal = (np.sin(x) + np.cos(y)) * 1000\n",
    "test_data = (signal + np.random.randn(nx, ny)*2).astype(np.float32)  # still mostly smooth\n",
    "\n",
    "compressions = {\n",
    "    'none': dict(compression=None, shuffle=False, scaleoffset=None),\n",
    "    'gzip': dict(compression='gzip', shuffle=True, scaleoffset=None),\n",
    "    'lzf':  dict(compression='lzf',  shuffle=True, scaleoffset=None),\n",
    "    # big wow (lossy): keep ~4 significant digits -> massive size drop, usually negligible error\n",
    "    'gzip+scaleoffset4': dict(compression='gzip', shuffle=True, scaleoffset=4),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "chunk = (256, 256)  # 2D chunks compress well for images/grids\n",
    "\n",
    "for name, kw in compressions.items():\n",
    "    fn = f'compression_{name}.h5'\n",
    "    if os.path.exists(fn): os.remove(fn)\n",
    "\n",
    "    # WRITE\n",
    "    t0 = time.time()\n",
    "    with h5py.File(fn, 'w') as f:\n",
    "        dset = f.create_dataset(\n",
    "            'data', data=test_data, chunks=chunk,\n",
    "            compression=kw['compression'],\n",
    "            shuffle=kw['shuffle'],\n",
    "            scaleoffset=kw['scaleoffset']  # None for lossless; int for quantization\n",
    "        )\n",
    "    write_time = time.time() - t0\n",
    "\n",
    "    # READ (cold-ish): reopen the file\n",
    "    t0 = time.time()\n",
    "    with h5py.File(fn, 'r') as f:\n",
    "        _ = f['data'][:]         # full read\n",
    "    read_time = time.time() - t0\n",
    "\n",
    "    size = os.path.getsize(fn)\n",
    "    ratio = test_data.nbytes / size\n",
    "    results[name] = (write_time, read_time, size/1024**2, ratio)\n",
    "\n",
    "print(\"HDF5 Compression Performance (structured float32, chunks=256x256)\")\n",
    "print(f\"{'Method':18s} {'Write(s)':>8s} {'Read(s)':>8s} {'Size(MB)':>9s} {'Ratio':>6s}\")\n",
    "print(\"-\"*60)\n",
    "for k,(wt,rt,mb,ra) in results.items():\n",
    "    print(f\"{k:18s} {wt:8.3f} {rt:8.3f} {mb:9.1f} {ra:6.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã HDF5 Quick Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import html\n",
    "\n",
    "html_block = \"\"\"\n",
    "<style>\n",
    ".h5-card {\n",
    "  font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;\n",
    "  max-width: 880px; \n",
    "  border:1px solid #ddd; \n",
    "  border-radius:12px; \n",
    "  padding:20px; \n",
    "  background: #fff; \n",
    "  box-shadow: 0 2px 10px rgba(0,0,0,.05);\n",
    "}\n",
    ".h5-card h2 {\n",
    "  margin-top:0;\n",
    "  font-size:22px;\n",
    "  display:flex;\n",
    "  align-items:center;\n",
    "  gap:8px;\n",
    "}\n",
    ".h5-card section {\n",
    "  margin-top:18px;\n",
    "}\n",
    ".h5-card h3 {\n",
    "  margin:0 0 6px;\n",
    "  font-size:16px;\n",
    "  color:#006699;\n",
    "  border-bottom:1px solid #eee;\n",
    "  padding-bottom:3px;\n",
    "}\n",
    ".h5-card pre {\n",
    "  background:#111; \n",
    "  color:#f2f2f2;\n",
    "  border-radius:6px; \n",
    "  padding:10px 12px; \n",
    "  overflow:auto;\n",
    "  line-height:1.35;\n",
    "  font-size:13px;\n",
    "}\n",
    ".copy-btn {\n",
    "  float:right;\n",
    "  margin-left:10px;\n",
    "  cursor:pointer;\n",
    "  border:1px solid #ccc;\n",
    "  background:white;\n",
    "  padding:4px 8px;\n",
    "  border-radius:6px;\n",
    "  font-size:12px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<div class=\"h5-card\" id=\"h5ref\">\n",
    "  <h2>üìö HDF5 Quick Reference \n",
    "    <button class=\"copy-btn\" onclick=\"navigator.clipboard.writeText(document.querySelector('#h5ref').innerText)\">Copy</button>\n",
    "  </h2>\n",
    "\n",
    "  <section>\n",
    "    <h3>Basic Operations</h3>\n",
    "    <pre>import h5py\n",
    "\n",
    "with h5py.File('data.h5', 'w') as f:\n",
    "    f.create_dataset('array', data=numpy_array)\n",
    "    group = f.create_group('experiment')\n",
    "    f.attrs['metadata'] = 'value'</pre>\n",
    "  </section>\n",
    "\n",
    "  <section>\n",
    "    <h3>Reading</h3>\n",
    "    <pre>with h5py.File('data.h5', 'r') as f:\n",
    "    data = f['array'][:]           # Read all\n",
    "    subset = f['array'][0:100]     # Partial read\n",
    "    metadata = f.attrs['metadata'] # Read attribute</pre>\n",
    "  </section>\n",
    "\n",
    "  <section>\n",
    "    <h3>Compression &amp; Performance</h3>\n",
    "    <pre>f.create_dataset('data', data=array,\n",
    "                 compression='gzip',     # gzip, lzf, szip\n",
    "                 compression_opts=9,     # Compression level\n",
    "                 chunks=True,            # Auto chunking\n",
    "                 shuffle=True)           # Byte shuffle filter</pre>\n",
    "  </section>\n",
    "\n",
    "  <section>\n",
    "    <h3>Groups &amp; Organization</h3>\n",
    "    <pre>experiment = f.create_group('experiment')\n",
    "experiment.create_dataset('data', data=array)\n",
    "experiment.attrs['researcher'] = 'Dr. Smith'</pre>\n",
    "  </section>\n",
    "\n",
    "  <section>\n",
    "    <h3>Advanced Features</h3>\n",
    "    <pre># Unlimited dimensions\n",
    "f.create_dataset('stream', shape=(0,), maxshape=(None,))\n",
    "\n",
    "# Compound data types\n",
    "dt = np.dtype([('name', 'S20'), ('value', 'f8')])\n",
    "f.create_dataset('records', data=structured_array, dtype=dt)</pre>\n",
    "  </section>\n",
    "\n",
    "  <section>\n",
    "    <h3>Best Practices</h3>\n",
    "    <ul style=\"margin:0; padding-left:18px; color:gray\">\n",
    "      <li>Use groups to organize related datasets</li>\n",
    "      <li>Add comprehensive metadata with attributes</li>\n",
    "      <li>Enable compression for most datasets</li>\n",
    "      <li>Use chunking for large arrays with partial access</li>\n",
    "      <li>Close files properly (use 'with' statements)</li>\n",
    "    </ul>\n",
    "  </section>\n",
    "\n",
    "  <div style=\"margin-top:15px;color:#444;font-size:13px;\">\n",
    "    üèóÔ∏è HDF5: The universal scientific data format! üìä\n",
    "  </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_block))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
