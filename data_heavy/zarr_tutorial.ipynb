{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install zarr and compression libraries\n",
    "!pip install zarr numpy matplotlib blosc lz4 zstandard numcodecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zarr Tutorial - Chunked & Compressed Array Storage\n",
    "\n",
    "**Zarr** is a library for chunked, compressed, N-dimensional arrays:\n",
    "- **Compression**: Dramatically reduces storage size (2-100x smaller)\n",
    "- **Chunking**: Efficient partial loading of massive arrays\n",
    "- **Performance**: Optimized I/O for scientific computing\n",
    "- **Compatibility**: Works with NumPy, Dask, and cloud storage\n",
    "\n",
    "Perfect for: satellite imagery, climate data, genomics, high-resolution simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numcodecs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Set up paths\n",
    "data_dir = Path('zarr_data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Zarr version: {zarr.__version__}\")\n",
    "print(f\"Data directory: {data_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Basic Zarr Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data of different types\n",
    "np.random.seed(42)\n",
    "\n",
    "# Different data patterns for compression testing\n",
    "sparse_data = np.random.choice([0, 1], size=(1000, 1000), p=[0.95, 0.05])  # Very sparse\n",
    "random_data = np.random.randn(1000, 1000).astype(np.float32)  # Random noise\n",
    "structured_data = np.sin(np.linspace(0, 20*np.pi, 1000))[:, None] * np.cos(np.linspace(0, 10*np.pi, 1000))\n",
    "integer_data = np.random.randint(0, 100, size=(1000, 1000), dtype=np.int32)\n",
    "\n",
    "datasets = {\n",
    "    'sparse': sparse_data,\n",
    "    'random': random_data,\n",
    "    'structured': structured_data.astype(np.float32),\n",
    "    'integer': integer_data\n",
    "}\n",
    "\n",
    "print(\"Sample datasets created:\")\n",
    "for name, data in datasets.items():\n",
    "    size_mb = data.nbytes / 1024**2\n",
    "    print(f\"  {name:10s}: {data.shape} {data.dtype} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different data types\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (name, data) in enumerate(datasets.items()):\n",
    "    im = axes[i].imshow(data[:200, :200], cmap='viridis')\n",
    "    axes[i].set_title(f'{name.title()} Data (200x200 subset)')\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóúÔ∏è Compression Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different compression algorithms\n",
    "compressors = {\n",
    "    'none': None,\n",
    "    'zlib': numcodecs.Zlib(level=6),\n",
    "    'blosc_lz4': numcodecs.Blosc(cname='lz4',  clevel=5, shuffle=numcodecs.Blosc.SHUFFLE),\n",
    "    'blosc_zstd': numcodecs.Blosc(cname='zstd', clevel=3, shuffle=numcodecs.Blosc.SHUFFLE),\n",
    "    'blosc_blosclz': numcodecs.Blosc(cname='blosclz', clevel=5, shuffle=numcodecs.Blosc.SHUFFLE),\n",
    "}\n",
    "\n",
    "def test_compression(data, name):\n",
    "    results = {}\n",
    "    original_size = data.nbytes\n",
    "    print(f\"\\nTesting compression on {name} data ({original_size/1024**2:.1f} MB):\")\n",
    "\n",
    "    # make chunk shape match ndim\n",
    "    chunks = tuple(min(100, s) for s in data.shape)\n",
    "\n",
    "    for comp_name, compressor in compressors.items():\n",
    "        store_path = data_dir / f'{name}_{comp_name}.zarr'\n",
    "        if store_path.exists():\n",
    "            shutil.rmtree(store_path)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        z = zarr.open(\n",
    "            str(store_path),\n",
    "            mode='w',\n",
    "            shape=data.shape,\n",
    "            dtype=data.dtype,\n",
    "            chunks=chunks,\n",
    "            compressor=compressor,   # valid for Zarr v2\n",
    "            zarr_format=2            # <-- force V2 to allow `compressor=`\n",
    "        )\n",
    "        z[:] = data\n",
    "        write_time = time.perf_counter() - t0\n",
    "\n",
    "        # size on disk\n",
    "        compressed_size = sum(f.stat().st_size for f in store_path.rglob('*') if f.is_file())\n",
    "        ratio = (original_size / compressed_size) if compressed_size else 0.0\n",
    "\n",
    "        # read speed\n",
    "        t0 = time.perf_counter()\n",
    "        _ = z[:]\n",
    "        read_time = time.perf_counter() - t0\n",
    "\n",
    "        results[comp_name] = dict(size=compressed_size, ratio=ratio,\n",
    "                                  write_time=write_time, read_time=read_time)\n",
    "\n",
    "        print(f\"  {comp_name:12s}: {compressed_size/1024**2:6.1f} MB \"\n",
    "              f\"({ratio:5.1f}x) W:{write_time:.3f}s R:{read_time:.3f}s\")\n",
    "    return results\n",
    "\n",
    "# run\n",
    "all_results = {name: test_compression(data, name) for name, data in datasets.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize compression results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Compression ratios\n",
    "comp_names = list(compressors.keys())[1:]  # Skip 'none'\n",
    "data_names = list(datasets.keys())\n",
    "\n",
    "ratios = np.array([[all_results[data][comp]['ratio'] for comp in comp_names] \n",
    "                   for data in data_names])\n",
    "\n",
    "im = axes[0,0].imshow(ratios, cmap='Reds')\n",
    "axes[0,0].set_xticks(range(len(comp_names)))\n",
    "axes[0,0].set_xticklabels(comp_names, rotation=45)\n",
    "axes[0,0].set_yticks(range(len(data_names)))\n",
    "axes[0,0].set_yticklabels(data_names)\n",
    "axes[0,0].set_title('Compression Ratios')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(data_names)):\n",
    "    for j in range(len(comp_names)):\n",
    "        axes[0,0].text(j, i, f'{ratios[i,j]:.1f}x', ha='center', va='center')\n",
    "\n",
    "plt.colorbar(im, ax=axes[0,0])\n",
    "\n",
    "# Write times\n",
    "write_times = np.array([[all_results[data][comp]['write_time'] for comp in comp_names] \n",
    "                        for data in data_names])\n",
    "\n",
    "im = axes[0,1].imshow(write_times, cmap='Blues')\n",
    "axes[0,1].set_xticks(range(len(comp_names)))\n",
    "axes[0,1].set_xticklabels(comp_names, rotation=45)\n",
    "axes[0,1].set_yticks(range(len(data_names)))\n",
    "axes[0,1].set_yticklabels(data_names)\n",
    "axes[0,1].set_title('Write Times (seconds)')\n",
    "plt.colorbar(im, ax=axes[0,1])\n",
    "\n",
    "# Read times\n",
    "read_times = np.array([[all_results[data][comp]['read_time'] for comp in comp_names] \n",
    "                       for data in data_names])\n",
    "\n",
    "im = axes[1,0].imshow(read_times, cmap='Greens')\n",
    "axes[1,0].set_xticks(range(len(comp_names)))\n",
    "axes[1,0].set_xticklabels(comp_names, rotation=45)\n",
    "axes[1,0].set_yticks(range(len(data_names)))\n",
    "axes[1,0].set_yticklabels(data_names)\n",
    "axes[1,0].set_title('Read Times (seconds)')\n",
    "plt.colorbar(im, ax=axes[1,0])\n",
    "\n",
    "# Storage sizes\n",
    "sizes = np.array([[all_results[data][comp]['size']/1024**2 for comp in comp_names] \n",
    "                  for data in data_names])\n",
    "\n",
    "im = axes[1,1].imshow(sizes, cmap='Purples')\n",
    "axes[1,1].set_xticks(range(len(comp_names)))\n",
    "axes[1,1].set_xticklabels(comp_names, rotation=45)\n",
    "axes[1,1].set_yticks(range(len(data_names)))\n",
    "axes[1,1].set_yticklabels(data_names)\n",
    "axes[1,1].set_title('Storage Size (MB)')\n",
    "plt.colorbar(im, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Chunking Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, zarr, shutil, time, numcodecs\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('./zarr_data')\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Test different chunking strategies\n",
    "large_data = np.random.randn(2000, 2000).astype(np.float32)\n",
    "\n",
    "chunk_strategies = {\n",
    "    'small': (50, 50),\n",
    "    'medium': (200, 200), \n",
    "    'large': (500, 500),\n",
    "    'row': (1, 2000),\n",
    "    'col': (2000, 1),\n",
    "    'auto': None  # Let zarr decide automatically\n",
    "}\n",
    "\n",
    "def test_chunking(data, chunk_size, name):\n",
    "    \"\"\"Test chunking strategy\"\"\"\n",
    "    store_path = data_dir / f'chunking_{name}.zarr'\n",
    "    if store_path.exists():\n",
    "        shutil.rmtree(store_path)\n",
    "\n",
    "    # Create with chunking\n",
    "    t0 = time.perf_counter()\n",
    "    z = zarr.open(\n",
    "        str(store_path), mode='w', shape=data.shape, dtype=data.dtype,\n",
    "        compressor=numcodecs.Blosc(cname='lz4', clevel=5),\n",
    "        chunks=chunk_size, zarr_format=2   # <-- force v2 here\n",
    "    )\n",
    "    z[:] = data\n",
    "    write_time = time.perf_counter() - t0\n",
    "\n",
    "    # Partial read\n",
    "    t0 = time.perf_counter()\n",
    "    subset = z[800:1200, 800:1200]\n",
    "    partial_read_time = time.perf_counter() - t0\n",
    "\n",
    "    # Full read\n",
    "    t0 = time.perf_counter()\n",
    "    full_data = z[:]\n",
    "    full_read_time = time.perf_counter() - t0\n",
    "\n",
    "    # Storage size\n",
    "    size = sum(f.stat().st_size for f in store_path.rglob('*') if f.is_file())\n",
    "\n",
    "    print(f\"Chunks {str(chunk_size):12s}: \"\n",
    "          f\"Write {write_time:.3f}s | \"\n",
    "          f\"Partial {partial_read_time:.3f}s | \"\n",
    "          f\"Full {full_read_time:.3f}s | \"\n",
    "          f\"Size {size/1024**2:.1f}MB\")\n",
    "\n",
    "    return dict(\n",
    "        chunks=z.chunks,\n",
    "        write_time=write_time,\n",
    "        partial_read_time=partial_read_time,\n",
    "        full_read_time=full_read_time,\n",
    "        size=size\n",
    "    )\n",
    "\n",
    "print(f\"Testing chunking on {large_data.shape} array \"\n",
    "      f\"({large_data.nbytes/1024**2:.1f} MB):\")\n",
    "print(\"Strategy        : Write Time | Partial Read | Full Read | Storage\")\n",
    "\n",
    "chunking_results = {}\n",
    "for name, chunks in chunk_strategies.items():\n",
    "    chunking_results[name] = test_chunking(large_data, chunks, name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Advanced Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-dimensional arrays and groups\n",
    "print(\"Creating multi-dimensional dataset...\")\n",
    "\n",
    "# Create a 4D dataset (time, z, y, x) - like climate data\n",
    "time_steps = 100\n",
    "z_levels = 20\n",
    "y_size = 200\n",
    "x_size = 200\n",
    "\n",
    "# Simulate climate data\n",
    "np.random.seed(42)\n",
    "temperature = 15 + 10 * np.sin(np.linspace(0, 4*np.pi, time_steps))[:, None, None, None] + \\\n",
    "              np.random.randn(time_steps, z_levels, y_size, x_size) * 2\n",
    "pressure = 1013 + np.random.randn(time_steps, z_levels, y_size, x_size) * 10\n",
    "humidity = np.random.beta(2, 2, (time_steps, z_levels, y_size, x_size)) * 100\n",
    "\n",
    "print(f\"Generated climate data: {temperature.shape} float32\")\n",
    "print(f\"Total size: {(temperature.nbytes + pressure.nbytes + humidity.nbytes)/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Zarr group (like HDF5 groups)\n",
    "group_path = data_dir / 'climate_data.zarr'\n",
    "if group_path.exists():\n",
    "    shutil.rmtree(group_path)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create root group\n",
    "root = zarr.open_group(str(group_path), mode='w')\n",
    "\n",
    "# Add metadata\n",
    "root.attrs['title'] = 'Climate Simulation Data'\n",
    "root.attrs['created'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "root.attrs['description'] = '4D climate data with time, z-level, lat, lon dimensions'\n",
    "\n",
    "# Create subgroups\n",
    "atmos = root.create_group('atmosphere')\n",
    "coords = root.create_group('coordinates')\n",
    "\n",
    "# Store main data with optimal chunking for time-series access\n",
    "temp_array = atmos.create_dataset('temperature', \n",
    "                                 shape=temperature.shape,\n",
    "                                 dtype=np.float32,\n",
    "                                 chunks=(10, 5, 50, 50),  # Time-friendly chunks\n",
    "                                 compressor=zarr.Blosc(cname='zstd', clevel=3))\n",
    "\n",
    "pres_array = atmos.create_dataset('pressure',\n",
    "                                 shape=pressure.shape,\n",
    "                                 dtype=np.float32,\n",
    "                                 chunks=(10, 5, 50, 50),\n",
    "                                 compressor=zarr.Blosc(cname='zstd', clevel=3))\n",
    "\n",
    "humid_array = atmos.create_dataset('humidity',\n",
    "                                  shape=humidity.shape,\n",
    "                                  dtype=np.float32,\n",
    "                                  chunks=(10, 5, 50, 50),\n",
    "                                  compressor=zarr.Blosc(cname='zstd', clevel=3))\n",
    "\n",
    "# Add coordinate arrays\n",
    "coords.array('time', np.arange(time_steps), chunks=50)\n",
    "coords.array('z_level', np.linspace(0, 20000, z_levels), chunks=20)  # meters\n",
    "coords.array('latitude', np.linspace(-90, 90, y_size), chunks=50)\n",
    "coords.array('longitude', np.linspace(-180, 180, x_size), chunks=50)\n",
    "\n",
    "# Write data\n",
    "temp_array[:] = temperature.astype(np.float32)\n",
    "pres_array[:] = pressure.astype(np.float32) \n",
    "humid_array[:] = humidity.astype(np.float32)\n",
    "\n",
    "# Add attributes to arrays\n",
    "temp_array.attrs['units'] = 'degrees_celsius'\n",
    "temp_array.attrs['long_name'] = 'Air Temperature'\n",
    "pres_array.attrs['units'] = 'hPa'\n",
    "pres_array.attrs['long_name'] = 'Air Pressure'\n",
    "humid_array.attrs['units'] = 'percent'\n",
    "humid_array.attrs['long_name'] = 'Relative Humidity'\n",
    "\n",
    "write_time = time.time() - start_time\n",
    "total_size = sum(f.stat().st_size for f in group_path.rglob('*') if f.is_file())\n",
    "original_size = temperature.nbytes + pressure.nbytes + humidity.nbytes\n",
    "\n",
    "print(f\"\\nClimate dataset created in {write_time:.2f}s:\")\n",
    "print(f\"  Original size: {original_size/1024**3:.2f} GB\")\n",
    "print(f\"  Compressed size: {total_size/1024**2:.1f} MB\")\n",
    "print(f\"  Compression ratio: {original_size/total_size:.1f}x\")\n",
    "print(f\"  Structure: {root.tree()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate efficient data access patterns\n",
    "print(\"Testing access patterns:\")\n",
    "\n",
    "# Reload the data\n",
    "root = zarr.open_group(str(group_path), mode='r')\n",
    "temp_data = root['atmosphere/temperature']\n",
    "\n",
    "# Time series at specific location\n",
    "start_time = time.time()\n",
    "time_series = temp_data[:, 10, 100, 100]  # All time, one z-level, one location\n",
    "time_access = time.time() - start_time\n",
    "\n",
    "# Spatial slice at specific time\n",
    "start_time = time.time()\n",
    "spatial_slice = temp_data[50, 10, :, :]  # One time, one z-level, all locations\n",
    "spatial_access = time.time() - start_time\n",
    "\n",
    "# Vertical profile at specific time/location\n",
    "start_time = time.time()\n",
    "profile = temp_data[50, :, 100, 100]  # One time, all z-levels, one location\n",
    "profile_access = time.time() - start_time\n",
    "\n",
    "print(f\"  Time series (100 points): {time_access:.3f}s\")\n",
    "print(f\"  Spatial slice (200x200): {spatial_access:.3f}s\")\n",
    "print(f\"  Vertical profile (20 levels): {profile_access:.3f}s\")\n",
    "\n",
    "# Visualize different access patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(time_series)\n",
    "axes[0].set_title('Time Series at Location (100,100,10)')\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Temperature (¬∞C)')\n",
    "\n",
    "im = axes[1].imshow(spatial_slice, cmap='RdYlBu_r')\n",
    "axes[1].set_title('Spatial Slice at Time=50, Z=10')\n",
    "plt.colorbar(im, ax=axes[1], label='Temperature (¬∞C)')\n",
    "\n",
    "axes[2].plot(profile, range(len(profile)))\n",
    "axes[2].set_title('Vertical Profile at (100,100,t=50)')\n",
    "axes[2].set_xlabel('Temperature (¬∞C)')\n",
    "axes[2].set_ylabel('Z Level')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Disk Usage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze disk usage of all created files\n",
    "def analyze_directory(path):\n",
    "    \"\"\"Analyze zarr directory structure and sizes\"\"\"\n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    \n",
    "    if path.exists():\n",
    "        for file_path in path.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                size = file_path.stat().st_size\n",
    "                total_size += size\n",
    "                file_count += 1\n",
    "    \n",
    "    return total_size, file_count\n",
    "\n",
    "print(\"Disk Usage Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_zarr_size = 0\n",
    "\n",
    "# Analyze individual compression tests\n",
    "for data_name in datasets.keys():\n",
    "    print(f\"\\n{data_name.title()} Data Compression:\")\n",
    "    original_mb = datasets[data_name].nbytes / 1024**2\n",
    "    \n",
    "    for comp_name in compressors.keys():\n",
    "        store_path = data_dir / f'{data_name}_{comp_name}.zarr'\n",
    "        size, files = analyze_directory(store_path)\n",
    "        total_zarr_size += size\n",
    "        \n",
    "        if size > 0:\n",
    "            ratio = datasets[data_name].nbytes / size\n",
    "            print(f\"  {comp_name:12s}: {size/1024**2:6.1f} MB ({files:3d} files) {ratio:5.1f}x compression\")\n",
    "\n",
    "# Analyze chunking tests\n",
    "print(f\"\\nChunking Strategy Analysis:\")\n",
    "original_mb = large_data.nbytes / 1024**2\n",
    "for strategy in chunk_strategies.keys():\n",
    "    store_path = data_dir / f'chunking_{strategy}.zarr'\n",
    "    size, files = analyze_directory(store_path)\n",
    "    total_zarr_size += size\n",
    "    \n",
    "    if size > 0:\n",
    "        print(f\"  {strategy:10s}: {size/1024**2:6.1f} MB ({files:3d} files)\")\n",
    "\n",
    "# Analyze climate dataset\n",
    "climate_size, climate_files = analyze_directory(group_path)\n",
    "total_zarr_size += climate_size\n",
    "print(f\"\\nClimate Dataset: {climate_size/1024**2:.1f} MB ({climate_files} files)\")\n",
    "\n",
    "print(f\"\\nTotal Zarr Storage: {total_zarr_size/1024**2:.1f} MB\")\n",
    "print(f\"Total Files Created: {sum(analyze_directory(data_dir)[1:])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance vs NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Zarr vs NumPy for large array operations\n",
    "print(\"Performance Comparison: Zarr vs NumPy\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create test data\n",
    "test_size = (2000, 2000)\n",
    "test_data = np.random.randn(*test_size).astype(np.float32)\n",
    "\n",
    "# NumPy save/load\n",
    "numpy_file = data_dir / 'test_numpy.npy'\n",
    "start_time = time.time()\n",
    "np.save(str(numpy_file), test_data)\n",
    "numpy_write_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "numpy_loaded = np.load(str(numpy_file))\n",
    "numpy_read_time = time.time() - start_time\n",
    "\n",
    "numpy_size = numpy_file.stat().st_size\n",
    "\n",
    "# Zarr save/load\n",
    "zarr_file = data_dir / 'test_zarr.zarr'\n",
    "if zarr_file.exists():\n",
    "    shutil.rmtree(zarr_file)\n",
    "\n",
    "start_time = time.time()\n",
    "z = zarr.open(str(zarr_file), mode='w', shape=test_data.shape, dtype=test_data.dtype,\n",
    "             compressor=zarr.Blosc(cname='lz4', clevel=5), chunks=(200, 200))\n",
    "z[:] = test_data\n",
    "zarr_write_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "zarr_loaded = z[:]\n",
    "zarr_read_time = time.time() - start_time\n",
    "\n",
    "zarr_size = sum(f.stat().st_size for f in zarr_file.rglob('*') if f.is_file())\n",
    "\n",
    "# Partial read test (central 400x400)\n",
    "start_time = time.time()\n",
    "numpy_partial = numpy_loaded[800:1200, 800:1200]\n",
    "numpy_partial_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "zarr_partial = z[800:1200, 800:1200]\n",
    "zarr_partial_time = time.time() - start_time\n",
    "\n",
    "# Results\n",
    "print(f\"Array size: {test_size} ({test_data.nbytes/1024**2:.1f} MB)\\n\")\n",
    "\n",
    "print(f\"{'Metric':20s} {'NumPy':>10s} {'Zarr':>10s} {'Speedup':>10s}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Write time (s)':20s} {numpy_write_time:>10.3f} {zarr_write_time:>10.3f} {zarr_write_time/numpy_write_time:>10.2f}x\")\n",
    "print(f\"{'Read time (s)':20s} {numpy_read_time:>10.3f} {zarr_read_time:>10.3f} {zarr_read_time/numpy_read_time:>10.2f}x\")\n",
    "print(f\"{'Partial read (s)':20s} {numpy_partial_time:>10.3f} {zarr_partial_time:>10.3f} {zarr_partial_time/numpy_partial_time:>10.2f}x\")\n",
    "print(f\"{'Storage (MB)':20s} {numpy_size/1024**2:>10.1f} {zarr_size/1024**2:>10.1f} {numpy_size/zarr_size:>10.1f}x\")\n",
    "\n",
    "print(f\"\\nüéØ Key Benefits:\")\n",
    "print(f\"   ‚Ä¢ Storage reduction: {numpy_size/zarr_size:.1f}x smaller\")\n",
    "print(f\"   ‚Ä¢ Partial reads: {numpy_partial_time/zarr_partial_time:.1f}x faster (chunked access)\")\n",
    "print(f\"   ‚Ä¢ Flexible compression: Multiple algorithms available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Zarr Quick Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up test files (optional)\n",
    "cleanup = input(\"Clean up test files? (y/n): \").lower().startswith('y')\n",
    "\n",
    "if cleanup:\n",
    "    shutil.rmtree(data_dir)\n",
    "    print(\"Test files cleaned up.\")\n",
    "else:\n",
    "    print(f\"Test files preserved in {data_dir.absolute()}\")\n",
    "\n",
    "reference = \"\"\"\n",
    "ZARR QUICK REFERENCE:\n",
    "\n",
    "Basic Operations:\n",
    "  import zarr\n",
    "  z = zarr.open('data.zarr', mode='w', shape=(1000, 1000), dtype='f4')\n",
    "  z[:] = data                    # Write data\n",
    "  loaded = z[:]                  # Read all\n",
    "  subset = z[100:200, 50:150]    # Read subset\n",
    "\n",
    "Compression:\n",
    "  zarr.Blosc(cname='lz4', clevel=5)       # Fast compression\n",
    "  zarr.Blosc(cname='zstd', clevel=3)      # Balanced\n",
    "  zarr.Blosc(cname='blosclz', clevel=9)   # High compression\n",
    "  zarr.Zlib(level=6)                      # Standard\n",
    "\n",
    "Chunking:\n",
    "  chunks=(100, 100)              # Fixed chunks\n",
    "  chunks=True                    # Auto chunks\n",
    "  chunks=(1, -1)                 # Row-wise chunks\n",
    "\n",
    "Groups (like HDF5):\n",
    "  root = zarr.open_group('data.zarr', mode='w')\n",
    "  root.create_dataset('array1', data=data)\n",
    "  group = root.create_group('subgroup')\n",
    "  root.attrs['metadata'] = 'value'\n",
    "\n",
    "Best Practices:\n",
    "  ‚Ä¢ Choose chunks based on access patterns\n",
    "  ‚Ä¢ Use appropriate data types (float32 vs float64)\n",
    "  ‚Ä¢ Test different compressors for your data\n",
    "  ‚Ä¢ Use groups for organizing related datasets\n",
    "  ‚Ä¢ Add metadata with .attrs dictionary\n",
    "\"\"\"\n",
    "\n",
    "print(reference)\n",
    "print(\"\\nüöÄ Zarr: Efficient storage for massive arrays! üíæ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
