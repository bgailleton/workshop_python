{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "warning-header",
   "metadata": {},
   "source": [
    "# ‚ö° Dask: Conquering MASSIVE Arrays ‚ö°\n",
    "\n",
    "## üî• PREPARE FOR COMPUTATIONAL POWER üî•\n",
    "\n",
    "**We're about to process a 30,000 √ó 30,000 digital elevation model (~7GB of data)**\n",
    "\n",
    "- üóª **900 MILLION elevation points** - larger than most regional DEMs\n",
    "- üßÆ **Complex terrain analysis** - gradients, aspects, curvature at massive scale\n",
    "- üìä **Multi-scale visualization** - from satellite view to hiking-trail detail\n",
    "- üíæ **Out-of-core processing** - dataset larger than typical RAM\n",
    "- ‚ö° **Parallel execution** - all CPU cores working simultaneously\n",
    "\n",
    "**System Requirements:** 4GB+ RAM, 2+ CPU cores, 15GB disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9c8f6-1fb7-4ead-adbf-81c5d2f4336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dask jupyter-server-proxy dask-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LightSource\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "import warnings\n",
    "from dask.distributed import Client, LocalCluster\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced plotting\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(f\"üöÄ System Specs:\")\n",
    "print(f\"   RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB\")\n",
    "print(f\"   CPU Cores: {psutil.cpu_count()}\")\n",
    "print(f\"   Dask Version: {dask.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "client-setup",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Unleashing Parallel Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dask-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Dask for maximum performance\n",
    "n_workers = min(6, psutil.cpu_count())\n",
    "memory_per_worker = max(1, int(psutil.virtual_memory().total / (n_workers * 1024**3) * 0.8))\n",
    "memory_limit = f'{memory_per_worker}GB'\n",
    "\n",
    "client = Client(\n",
    "    n_workers=n_workers,\n",
    "    threads_per_worker=2,\n",
    "    memory_limit=memory_limit,\n",
    "    dashboard_address=':8787',\n",
    "    silence_logs=False\n",
    ")\n",
    "\n",
    "print(f\"üî• Dask Cluster Online:\")\n",
    "print(f\"   Workers: {n_workers}\")\n",
    "print(f\"   Total Threads: {n_workers * 2}\")\n",
    "print(f\"   Memory per Worker: {memory_limit}\")\n",
    "print(f\"   Dashboard: {client.dashboard_link}\")\n",
    "print(f\"\\n‚ö° Ready to process multi-GB datasets! ‚ö°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terrain-creation",
   "metadata": {},
   "source": [
    "## üóª Generating Realistic Massive Terrain\n",
    "\n",
    "Creating a **30,000 √ó 30,000** realistic digital elevation model using multi-octave noise.\n",
    "This represents a ~300km √ó 300km area at 10m resolution - **larger than Belgium!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terrain-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massive array configuration\n",
    "ARRAY_SIZE = (30000, 30000)  # 900 million points!\n",
    "CHUNK_SIZE = (1500, 1500)   # 2.25M points per chunk = ~18MB\n",
    "PIXEL_SIZE = 10.0           # 10 meters per pixel\n",
    "EXTENT_KM = (ARRAY_SIZE[0] * PIXEL_SIZE) / 1000  # Total extent in km\n",
    "\n",
    "print(f\"üóª Creating MASSIVE Terrain:\")\n",
    "print(f\"   Grid Size: {ARRAY_SIZE[0]:,} √ó {ARRAY_SIZE[1]:,} = {np.prod(ARRAY_SIZE)/1e6:.0f} million points\")\n",
    "print(f\"   Real-world Size: {EXTENT_KM:.0f}km √ó {EXTENT_KM:.0f}km\")\n",
    "print(f\"   Data Volume: ~{np.prod(ARRAY_SIZE) * 8 / 1024**3:.1f} GB\")\n",
    "print(f\"   Chunks: {ARRAY_SIZE[0]//CHUNK_SIZE[0]} √ó {ARRAY_SIZE[1]//CHUNK_SIZE[1]} = {(ARRAY_SIZE[0]//CHUNK_SIZE[0]) * (ARRAY_SIZE[1]//CHUNK_SIZE[1])} chunks\")\n",
    "\n",
    "def generate_realistic_terrain(shape, chunks, pixel_size=10.0):\n",
    "    \"\"\"Generate ultra-realistic terrain using multi-scale features.\n",
    "    \n",
    "    Creates terrain with:\n",
    "    - Mountain ranges (large scale structure)\n",
    "    - Valley networks (medium scale drainage)\n",
    "    - Surface roughness (small scale detail)\n",
    "    - Realistic elevation distribution\n",
    "    \"\"\"\n",
    "    print(\"   üèîÔ∏è  Generating mountain ranges...\")\n",
    "    \n",
    "    # Create coordinate grids (normalized 0-1)\n",
    "    x = da.linspace(0, 1, shape[1], chunks=chunks[1])\n",
    "    y = da.linspace(0, 1, shape[0], chunks=chunks[0])\n",
    "    X, Y = da.meshgrid(x, y, indexing='ij')\n",
    "    \n",
    "    print(\"   üèûÔ∏è  Adding large-scale topography...\")\n",
    "    # Large scale: Continental features (wavelength ~100km)\n",
    "    large_scale = 2000 * da.sin(X * 4 * np.pi) * da.cos(Y * 3 * np.pi)\n",
    "    large_scale += 1500 * da.cos(X * 6 * np.pi + np.pi/4) * da.sin(Y * 4 * np.pi)\n",
    "    \n",
    "    print(\"   ‚õ∞Ô∏è  Creating mountain ranges...\")\n",
    "    # Mountain ranges: Gaussian peaks\n",
    "    range1 = 3000 * da.exp(-((X - 0.3)**2 + (Y - 0.7)**2) / 0.05)\n",
    "    range2 = 2500 * da.exp(-((X - 0.7)**2 + (Y - 0.3)**2) / 0.03)\n",
    "    range3 = 2000 * da.exp(-((X - 0.5)**2 + (Y - 0.5)**2) / 0.08)\n",
    "    \n",
    "    # Ridge lines\n",
    "    ridge1 = 1000 * da.exp(-da.minimum((Y - 0.2 - 0.3*X)**2 / 0.001, 5))\n",
    "    ridge2 = 800 * da.exp(-da.minimum((Y - 0.8 + 0.2*X)**2 / 0.001, 5))\n",
    "    \n",
    "    print(\"   üåä  Adding medium-scale features...\")\n",
    "    # Medium scale: Valley networks (wavelength ~10km)\n",
    "    medium_scale = 800 * da.sin(X * 20 * np.pi) * da.cos(Y * 18 * np.pi)\n",
    "    medium_scale += 600 * da.cos(X * 25 * np.pi) * da.sin(Y * 22 * np.pi)\n",
    "    \n",
    "    print(\"   üèïÔ∏è  Adding fine-scale detail...\")\n",
    "    # Small scale: Local topography (wavelength ~1km)\n",
    "    small_scale = 200 * da.sin(X * 100 * np.pi) * da.cos(Y * 95 * np.pi)\n",
    "    small_scale += 150 * da.cos(X * 120 * np.pi) * da.sin(Y * 110 * np.pi)\n",
    "    \n",
    "    print(\"   üåø  Adding surface roughness...\")\n",
    "    # Surface roughness: Random variations\n",
    "    roughness = da.random.normal(0, 25, shape, chunks=chunks)\n",
    "    \n",
    "    print(\"   üèóÔ∏è  Assembling final terrain...\")\n",
    "    # Combine all scales\n",
    "    elevation = (large_scale + range1 + range2 + range3 + ridge1 + ridge2 + \n",
    "                medium_scale + small_scale + roughness)\n",
    "    \n",
    "    # Add base elevation and ensure no negative values\n",
    "    elevation = da.maximum(elevation + 500, 0)\n",
    "    \n",
    "    return elevation\n",
    "\n",
    "# Generate the massive terrain\n",
    "print(\"\\nüöÄ GENERATING MASSIVE TERRAIN...\")\n",
    "start_time = time.time()\n",
    "\n",
    "massive_terrain = generate_realistic_terrain(ARRAY_SIZE, CHUNK_SIZE, PIXEL_SIZE)\n",
    "\n",
    "generation_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Terrain computation graph built in {generation_time:.3f}s\")\n",
    "print(f\"üìä Array Details: {massive_terrain}\")\n",
    "print(f\"üíæ Memory footprint: {massive_terrain.nbytes / 1024**3:.1f} GB (when computed)\")\n",
    "print(f\"\\n‚ö° Ready for MASSIVE parallel processing! ‚ö°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-computations",
   "metadata": {},
   "source": [
    "## üßÆ Advanced Terrain Analysis at Scale\n",
    "\n",
    "Computing **slope, aspect, and curvature** across 900 million elevation points using proper finite differences with boundary handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terrain-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advanced_terrain_metrics(elevation, pixel_size=10.0):\n",
    "    \"\"\"Compute comprehensive terrain metrics using proper finite differences.\n",
    "    \n",
    "    Returns slope, aspect, curvature using map_overlap for correct boundary handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def terrain_analysis_block(block):\n",
    "        \"\"\"Analyze terrain metrics for a single block with proper boundary handling.\"\"\"\n",
    "        # Compute gradients using central differences\n",
    "        gy, gx = np.gradient(block, pixel_size, pixel_size)\n",
    "        \n",
    "        # Slope (magnitude of gradient vector)\n",
    "        slope = np.sqrt(gx**2 + gy**2)\n",
    "        \n",
    "        # Aspect (direction of steepest descent)\n",
    "        # Use proper downslope direction convention\n",
    "        aspect = np.arctan2(-gy, -gx)  # Negative for downslope\n",
    "        aspect = np.where(aspect < 0, aspect + 2*np.pi, aspect)  # Convert to 0-2œÄ\n",
    "        \n",
    "        # Second derivatives for curvature\n",
    "        gyy, gyx = np.gradient(gy, pixel_size, pixel_size)\n",
    "        gxy, gxx = np.gradient(gx, pixel_size, pixel_size)\n",
    "        \n",
    "        # Mean curvature (simplified)\n",
    "        mean_curvature = -(gxx + gyy) / 2.0\n",
    "        \n",
    "        # Stack all results\n",
    "        return np.stack([slope, aspect, mean_curvature], axis=0)\n",
    "    \n",
    "    print(\"   üî¨ Computing terrain derivatives with boundary handling...\")\n",
    "    # Apply to all chunks with overlapping boundaries to avoid edge artifacts\n",
    "    results = da.map_overlap(\n",
    "        terrain_analysis_block,\n",
    "        elevation,\n",
    "        depth=2,  # 2-pixel overlap for gradient computation\n",
    "        boundary='reflect',  # Reflect at boundaries\n",
    "        dtype=np.float64,\n",
    "        new_axis=0,  # Add new axis for different metrics\n",
    "        chunks=(3,) + elevation.chunks  # 3 metrics + original chunk structure\n",
    "    )\n",
    "    \n",
    "    # Split into individual arrays\n",
    "    slope = results[0]\n",
    "    aspect = results[1] \n",
    "    curvature = results[2]\n",
    "    \n",
    "    return {\n",
    "        'slope': slope,\n",
    "        'aspect': aspect,\n",
    "        'curvature': curvature\n",
    "    }\n",
    "\n",
    "print(\"üßÆ COMPUTING ADVANCED TERRAIN ANALYSIS...\")\n",
    "print(f\"   Processing {np.prod(ARRAY_SIZE)/1e6:.0f} million elevation points\")\n",
    "print(f\"   Computing: Slope, Aspect, Curvature with proper boundary handling\")\n",
    "\n",
    "analysis_start = time.time()\n",
    "\n",
    "# Compute all terrain metrics\n",
    "terrain_metrics = compute_advanced_terrain_metrics(massive_terrain, PIXEL_SIZE)\n",
    "\n",
    "setup_time = time.time() - analysis_start\n",
    "print(f\"\\n‚úÖ Analysis computation graph built in {setup_time:.3f}s\")\n",
    "\n",
    "# Show what we've created\n",
    "for name, array in terrain_metrics.items():\n",
    "    print(f\"   üìä {name}: {array.shape} @ {array.dtype}\")\n",
    "\n",
    "print(f\"\\nüìà Total data volume: {sum(arr.nbytes for arr in terrain_metrics.values()) / 1024**3:.1f} GB\")\n",
    "print(f\"‚è±Ô∏è  Ready for parallel execution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-computation",
   "metadata": {},
   "source": [
    "## ‚ö° EXECUTING MASSIVE PARALLEL COMPUTATION\n",
    "\n",
    "Time to **unleash the full power** of parallel processing on our massive dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Massive Dask Terrain Analysis ‚Äî drop-in ready ===\n",
    "import os, time\n",
    "import numpy as np\n",
    "import psutil\n",
    "import dask.array as da\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "ARRAY_SIZE = (12000, 12000)     # ~144M pts\n",
    "CHUNK_SIZE = (1500, 1500)       # tune for RAM/cores\n",
    "CELL_SIZE  = 1.0                # meters\n",
    "PERCENTILE_METHOD = \"tdigest\"   # \"tdigest\" or \"dask\" preferred; auto-fallback handled\n",
    "SEED = 42\n",
    "\n",
    "# -----------------------------\n",
    "# Dask cluster\n",
    "# -----------------------------\n",
    "cluster = LocalCluster(\n",
    "    threads_per_worker=2,\n",
    "    n_workers=max(1, os.cpu_count() // 4),\n",
    "    processes=True,\n",
    "    dashboard_address=None,\n",
    ")\n",
    "client = Client(cluster)\n",
    "\n",
    "# -----------------------------\n",
    "# Gaussian helper (dask-image if available; else SciPy via map_overlap)\n",
    "# -----------------------------\n",
    "try:\n",
    "    from dask_image.ndfilters import gaussian_filter as _da_gaussian\n",
    "    def dask_gaussian(x, sigma):\n",
    "        y = _da_gaussian(x, sigma=sigma)\n",
    "        return y.astype(x.dtype, copy=False)\n",
    "except Exception:\n",
    "    import scipy.ndimage as ndi\n",
    "    def dask_gaussian(x, sigma):\n",
    "        depth = int(3 * sigma)\n",
    "        return da.map_overlap(\n",
    "            ndi.gaussian_filter, x,\n",
    "            sigma=sigma, depth=depth, boundary='reflect', dtype=x.dtype,\n",
    "        )\n",
    "\n",
    "# RNG (Dask uses size=..., no dtype kw)\n",
    "rs = da.random.RandomState(SEED)\n",
    "def smooth_field(shape, chunks, scale):\n",
    "    return rs.random(size=shape, chunks=chunks).astype(np.float32) * scale\n",
    "\n",
    "# -----------------------------\n",
    "# Build terrain (ensure multi-chunk for tdigest)\n",
    "# -----------------------------\n",
    "massive_terrain = (\n",
    "    dask_gaussian(smooth_field(ARRAY_SIZE, CHUNK_SIZE, 300.0), sigma=8)\n",
    "  + dask_gaussian(smooth_field(ARRAY_SIZE, CHUNK_SIZE, 150.0), sigma=16)\n",
    "  + dask_gaussian(smooth_field(ARRAY_SIZE, CHUNK_SIZE,  50.0), sigma=32)\n",
    ").astype(np.float32).rechunk(CHUNK_SIZE)\n",
    "\n",
    "# -----------------------------\n",
    "# Terrain metrics\n",
    "# -----------------------------\n",
    "gy, gx = da.gradient(massive_terrain, CELL_SIZE)   # returns d/dy, d/dx\n",
    "slope = da.sqrt(gx**2 + gy**2)\n",
    "aspect = da.arctan2(-gy, -gx)                      # downslope aspect\n",
    "dgy_dy, dgy_dx = da.gradient(gy, CELL_SIZE)\n",
    "dgx_dy, dgx_dx = da.gradient(gx, CELL_SIZE)\n",
    "curvature = (dgx_dx + dgy_dy)                      # Laplacian-like\n",
    "terrain_metrics = {\"slope\": slope, \"aspect\": aspect, \"curvature\": curvature}\n",
    "\n",
    "# -----------------------------\n",
    "# Robust global percentiles helper\n",
    "# -----------------------------\n",
    "def global_percentiles(a, qs=(25, 50, 75), method=\"midpoint\"):\n",
    "    a1 = a.ravel()\n",
    "    # Count total chunks; if it's 1, NumPy will be used -> no \"tdigest\"\n",
    "    n_chunks = 1\n",
    "    for c in a1.chunks:\n",
    "        n_chunks *= len(c)\n",
    "    use_method = method if (n_chunks > 1 and method in (\"tdigest\", \"dask\")) else \"linear\"\n",
    "    return da.percentile(a1, qs, method=use_method)\n",
    "\n",
    "# -----------------------------\n",
    "# Pretty header + system before\n",
    "# -----------------------------\n",
    "print(\"‚ö° LAUNCHING MASSIVE PARALLEL COMPUTATION ‚ö°\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "memory_before = psutil.virtual_memory()\n",
    "cpu_before = psutil.cpu_percent(interval=1)\n",
    "print(f\"üñ•Ô∏è  System Status Before:\")\n",
    "print(f\"   RAM: {memory_before.used/1024**3:.1f}/{memory_before.total/1024**3:.1f} GB ({memory_before.percent:.1f}%)\")\n",
    "print(f\"   CPU: {cpu_before:.1f}%\")\n",
    "\n",
    "# -----------------------------\n",
    "# Phase 1: elevation stats\n",
    "# -----------------------------\n",
    "print(f\"\\nüîÑ Phase 1: Computing elevation statistics...\")\n",
    "print(f\"\\nüîÑ Phase 1: Computing elevation statistics...\")\n",
    "stats_start = time.time()\n",
    "\n",
    "def safe_global_percentiles(arr, qs=(25, 50, 75), preferred=\"tdigest\"):\n",
    "    \"\"\"Use Dask percentiles; if NumPy gets invoked (single-chunk) and pukes on 'tdigest',\n",
    "    auto-fallback to a NumPy-supported method.\"\"\"\n",
    "    a1 = arr.rechunk(CHUNK_SIZE).ravel()  # ensure multi-chunk before ravel (helps Dask paths)\n",
    "    try:\n",
    "        p = da.percentile(a1, qs, method=preferred)\n",
    "        # try computing alone first to trigger early if it will fall back to NumPy\n",
    "        (p_vals,) = da.compute(p)\n",
    "    except Exception:\n",
    "        # fall back to a NumPy-compatible method\n",
    "        p = da.percentile(a1, qs, method=\"linear\")\n",
    "        (p_vals,) = da.compute(p)\n",
    "    return p_vals  # numpy array of quantiles\n",
    "\n",
    "q1, q2, q3 = safe_global_percentiles(massive_terrain, (25, 50, 75), preferred=PERCENTILE_METHOD)\n",
    "\n",
    "elev_stats = da.compute(\n",
    "    massive_terrain.min(),\n",
    "    massive_terrain.max(),\n",
    "    massive_terrain.mean(),\n",
    "    massive_terrain.std(),\n",
    ")\n",
    "stats_time = time.time() - stats_start\n",
    "\n",
    "min_elev, max_elev, mean_elev, std_elev = elev_stats\n",
    "print(f\"‚úÖ Elevation analysis completed in {stats_time:.2f}s\")\n",
    "print(f\"üìä Elevation Statistics:\")\n",
    "print(f\"   Range: {min_elev:.1f} - {max_elev:.1f} m ({max_elev-min_elev:.1f} m relief)\")\n",
    "print(f\"   Mean ¬± Std: {mean_elev:.1f} ¬± {std_elev:.1f} m\")\n",
    "print(f\"   Quartiles: Q1={q1:.0f} Q2={q2:.0f} Q3={q3:.0f}\")\n",
    "\n",
    "stats_time = time.time() - stats_start\n",
    "min_elev, max_elev, mean_elev, std_elev = elev_stats\n",
    "print(f\"‚úÖ Elevation analysis completed in {stats_time:.2f}s\")\n",
    "print(f\"üìä Elevation Statistics:\")\n",
    "print(f\"   Range: {min_elev:.1f} - {max_elev:.1f} m ({max_elev-min_elev:.1f} m relief)\")\n",
    "print(f\"   Mean ¬± Std: {mean_elev:.1f} ¬± {std_elev:.1f} m\")\n",
    "print(f\"   Quartiles: Q1={q1:.0f} Q2={q2:.0f} Q3={q3:.0f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Phase 2: terrain metric stats\n",
    "# -----------------------------\n",
    "n_workers = len(client.scheduler_info()['workers'])\n",
    "print(f\"\\nüî• Phase 2: MASSIVE terrain analysis computation...\")\n",
    "print(f\"   üéØ Target: {len(terrain_metrics)} metrics √ó {np.prod(ARRAY_SIZE)/1e6:.0f} million points each\")\n",
    "print(f\"   ‚ö° Workers: {n_workers} parallel workers\")\n",
    "terrain_start = time.time()\n",
    "\n",
    "slope_stats = da.compute(\n",
    "    terrain_metrics['slope'].mean(),\n",
    "    terrain_metrics['slope'].max(),\n",
    "    terrain_metrics['slope'].std(),\n",
    "    (terrain_metrics['slope'] > np.tan(np.deg2rad(45.0))).sum()\n",
    ")\n",
    "aspect_stats = da.compute(\n",
    "    terrain_metrics['aspect'].mean(),\n",
    "    terrain_metrics['aspect'].std()\n",
    ")\n",
    "curvature_stats = da.compute(\n",
    "    terrain_metrics['curvature'].mean(),\n",
    "    terrain_metrics['curvature'].std(),\n",
    "    (terrain_metrics['curvature'] > 0).sum(),\n",
    "    (terrain_metrics['curvature'] < 0).sum()\n",
    ")\n",
    "terrain_time = time.time() - terrain_start\n",
    "\n",
    "slope_mean, slope_max, slope_std, steep_count = slope_stats\n",
    "aspect_mean, aspect_std = aspect_stats\n",
    "curv_mean, curv_std, convex_count, concave_count = curvature_stats\n",
    "\n",
    "# -----------------------------\n",
    "# Summary\n",
    "# -----------------------------\n",
    "print(f\"\\nüéâ MASSIVE COMPUTATION COMPLETED! üéâ\")\n",
    "print(f\"‚è±Ô∏è  Total computation time: {terrain_time:.2f}s\")\n",
    "bytes_processed = 4 * np.prod(ARRAY_SIZE) * 8  # heuristic\n",
    "print(f\"üöÄ Processing rate: {(bytes_processed / 1024**3) / terrain_time:.2f} GB/s\")\n",
    "memory_after = psutil.virtual_memory()\n",
    "print(f\"üíæ Memory usage: {memory_after.used/1024**3:.1f} GB (+{(memory_after.used-memory_before.used)/1024**3:.1f} GB)\")\n",
    "\n",
    "print(f\"\\nüìà TERRAIN ANALYSIS RESULTS:\")\n",
    "print(\"‚îÅ\" * 40)\n",
    "print(f\"üèîÔ∏è  Slope Analysis:\")\n",
    "print(f\"   Mean slope: {slope_mean:.3f} ({np.degrees(np.arctan(slope_mean)):.1f}¬∞)\")\n",
    "print(f\"   Max slope:  {slope_max:.3f} ({np.degrees(np.arctan(slope_max)):.1f}¬∞)\")\n",
    "print(f\"   Steep areas (>45¬∞): {steep_count:,} points ({steep_count/np.prod(ARRAY_SIZE)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüß≠ Aspect Analysis:\")\n",
    "print(f\"   Mean aspect: {aspect_mean:.3f} rad ({np.degrees(aspect_mean):.1f}¬∞)\")\n",
    "print(f\"   Aspect variability: {aspect_std:.3f} rad\")\n",
    "\n",
    "print(f\"\\nüåä Curvature Analysis:\")\n",
    "print(f\"   Mean curvature: {curv_mean:.2e} m‚Åª¬π\")\n",
    "print(f\"   Convex areas: {convex_count:,} points ({convex_count/np.prod(ARRAY_SIZE)*100:.1f}%)\")\n",
    "print(f\"   Concave areas: {concave_count:,} points ({concave_count/np.prod(ARRAY_SIZE)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüèÜ ACHIEVEMENT: Processed {np.prod(ARRAY_SIZE)/1e6:.0f} million points! üèÜ\")\n",
    "# === end ===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiscale-viz",
   "metadata": {},
   "source": [
    "## üîç MULTI-SCALE VISUALIZATION SPECTACULAR\n",
    "\n",
    "**The grand finale!** Multi-level zoom visualization showing the **full power** of our massive computation.\n",
    "\n",
    "From **satellite overview** (300km view) to **hiking detail** (5km view) - all from the same massive dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiscale-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, numpy as np, dask.array as da\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LightSource\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def create_multiscale_visualization(elevation, slope, aspect, curvature, array_size, pixel_size):\n",
    "    \"\"\"Multi-scale terrain viz with correct nested zooms, global km axes, and next-zoom red boxes.\"\"\"\n",
    "    print(\"üé® CREATING SPECTACULAR MULTI-SCALE VISUALIZATION\")\n",
    "    print(\"‚îÅ\" * 55)\n",
    "\n",
    "    ps_km = pixel_size / 1000.0\n",
    "    full_x_km = array_size[1] * ps_km\n",
    "    full_y_km = array_size[0] * ps_km\n",
    "    EXTENT_KM = min(full_x_km, full_y_km)\n",
    "\n",
    "    # km -> pixels (rounded)\n",
    "    def km_to_px(km):\n",
    "        return int(round(km / ps_km))\n",
    "\n",
    "    # Center everything on the domain center to guarantee nesting\n",
    "    cy, cx = array_size[0] // 2, array_size[1] // 2\n",
    "\n",
    "    # Desired zoom sizes in km (Full, 100, 30, 2)\n",
    "    zoom_defs = [\n",
    "        dict(name='üó∫Ô∏è  REGIONAL VIEW', size_km=None, downsample=30,\n",
    "             desc=lambda: f'{EXTENT_KM:.0f}km √ó {EXTENT_KM:.0f}km - Full Dataset'),\n",
    "        # dict(name='üó∫Ô∏è  REGIONAL VIEW',  size_km=100.0, downsample=8,\n",
    "        #      desc=lambda: '100km √ó 100km - Regional'),\n",
    "        dict(name='üèîÔ∏è  VALLEY VIEW',    size_km=30.0,  downsample=3,\n",
    "             desc=lambda: '30km √ó 30km - Valley'),\n",
    "        dict(name='ü•æ HIKING DETAIL',    size_km=2.0,   downsample=1,\n",
    "             desc=lambda: '2km √ó 2km - Local'),\n",
    "    ]\n",
    "\n",
    "    # Compute pixel windows, nested & clipped to domain\n",
    "    zoom_levels = []\n",
    "    prev_window = (0, array_size[0], 0, array_size[1])  # y0,y1,x0,x1 for Full\n",
    "    for i, z in enumerate(zoom_defs):\n",
    "        if z['size_km'] is None:\n",
    "            y0, y1, x0, x1 = prev_window  # full domain\n",
    "        else:\n",
    "            half = km_to_px(z['size_km']) // 2\n",
    "            # start from global center\n",
    "            y0 = cy - half; y1 = cy + half\n",
    "            x0 = cx - half; x1 = cx + half\n",
    "            # clip to domain\n",
    "            y0 = max(0, y0); y1 = min(array_size[0], y1)\n",
    "            x0 = max(0, x0); x1 = min(array_size[1], x1)\n",
    "            # ensure nesting inside previous window\n",
    "            py0, py1, px0, px1 = prev_window\n",
    "            if y0 < py0: y0 = py0\n",
    "            if y1 > py1: y1 = py1\n",
    "            if x0 < px0: x0 = px0\n",
    "            if x1 > px1: x1 = px1\n",
    "            # if still too small due to borders, shrink as needed (keeps nesting)\n",
    "            y0 = int(y0); y1 = int(y1); x0 = int(x0); x1 = int(x1)\n",
    "        window = (int(y0), int(y1), int(x0), int(x1))\n",
    "        zoom_levels.append(dict(\n",
    "            name=z['name'], description=z['desc'](),\n",
    "            downsample=z['downsample'], window=window\n",
    "        ))\n",
    "        prev_window = window  # next must be inside this\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    print(f\"üìä Extracting data for {len(zoom_levels)} zoom levels...\")\n",
    "\n",
    "    extracted = []\n",
    "    for lvl in zoom_levels:\n",
    "        y0, y1, x0, x1 = lvl['window']\n",
    "        ds = lvl['downsample']\n",
    "        t0 = time.time()\n",
    "\n",
    "        elev_subset = elevation[y0:y1, x0:x1][::ds, ::ds]\n",
    "        slope_subset = slope[y0:y1, x0:x1][::ds, ::ds]\n",
    "        aspect_subset = aspect[y0:y1, x0:x1][::ds, ::ds]\n",
    "        curv_subset = curvature[y0:y1, x0:x1][::ds, ::ds]\n",
    "        elev, slp, asp, curv = da.compute(elev_subset, slope_subset, aspect_subset, curv_subset)\n",
    "\n",
    "        print(f\"   üîç {lvl['name']}: {lvl['description']}  \"\n",
    "              f\"win[y:{y0}:{y1}, x:{x0}:{x1}], ds={ds} ‚Üí shape {elev.shape}  \"\n",
    "              f\"in {time.time()-t0:.2f}s\")\n",
    "\n",
    "        # Global coordinates extent in km for imshow\n",
    "        extent_km = [x0 * ps_km, x1 * ps_km, y0 * ps_km, y1 * ps_km]\n",
    "        extracted.append(dict(\n",
    "            elevation=elev, slope=slp, aspect=asp, curvature=curv,\n",
    "            extent_km=extent_km, ds=ds, level=lvl\n",
    "        ))\n",
    "\n",
    "    print(\"\\nüé® Creating visualization panels...\")\n",
    "\n",
    "    def draw_next_extent(ax, nxt_window):\n",
    "        y0n, y1n, x0n, x1n = nxt_window\n",
    "        rect = Rectangle((x0n * ps_km, y0n * ps_km),\n",
    "                         (x1n - x0n) * ps_km, (y1n - y0n) * ps_km,\n",
    "                         fill=False, lw=2.0, edgecolor='red')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    def style_axes(ax):\n",
    "        ax.set_xlabel('x (km)')\n",
    "        ax.set_ylabel('y (km)')\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=5))\n",
    "\n",
    "    # 4 columns per level: Elevation, Slope, Aspect, Curvature\n",
    "    for i, data in enumerate(extracted):\n",
    "        # Elevation + hillshade\n",
    "        ax_e = plt.subplot(len(zoom_levels), 4, i*4 + 1)\n",
    "        ls = LightSource(azdeg=315, altdeg=45)\n",
    "        try:\n",
    "            hill = ls.hillshade(data['elevation'], vert_exag=2)\n",
    "            ax_e.imshow(hill, cmap='gray', alpha=0.30, origin='lower',\n",
    "                        extent=data['extent_km'])\n",
    "        except Exception:\n",
    "            pass\n",
    "        im_e = ax_e.imshow(data['elevation'], cmap='terrain', alpha=0.80, origin='lower',\n",
    "                           extent=data['extent_km'])\n",
    "        ax_e.set_title(f\"{data['level']['name']}\\n{data['level']['description']}\",\n",
    "                       fontsize=10, fontweight='bold')\n",
    "        style_axes(ax_e)\n",
    "        plt.colorbar(im_e, ax=ax_e, fraction=0.046, pad=0.04, label='Elevation (m)')\n",
    "\n",
    "        # Slope (deg)\n",
    "        ax_s = plt.subplot(len(zoom_levels), 4, i*4 + 2)\n",
    "        slope_deg = np.degrees(np.arctan(data['slope']))\n",
    "        im_s = ax_s.imshow(slope_deg, cmap='plasma', vmin=0, vmax=45, origin='lower',\n",
    "                           extent=data['extent_km'])\n",
    "        ax_s.set_title('Slope (degrees)', fontsize=10)\n",
    "        style_axes(ax_s)\n",
    "        plt.colorbar(im_s, ax=ax_s, fraction=0.046, pad=0.04, label='Slope (¬∞)')\n",
    "\n",
    "        # Aspect (deg in [0,360))\n",
    "        ax_a = plt.subplot(len(zoom_levels), 4, i*4 + 3)\n",
    "        aspect_deg = (np.degrees(data['aspect']) + 360.0) % 360.0\n",
    "        im_a = ax_a.imshow(aspect_deg, cmap='hsv', vmin=0, vmax=360, origin='lower',\n",
    "                           extent=data['extent_km'])\n",
    "        ax_a.set_title('Aspect (degrees)', fontsize=10)\n",
    "        style_axes(ax_a)\n",
    "        plt.colorbar(im_a, ax=ax_a, fraction=0.046, pad=0.04, label='Aspect (¬∞)')\n",
    "\n",
    "        # Curvature (scaled)\n",
    "        ax_c = plt.subplot(len(zoom_levels), 4, i*4 + 4)\n",
    "        curv_scaled = data['curvature'] * 1000.0\n",
    "        curv_max = np.percentile(np.abs(curv_scaled), 95)\n",
    "        im_c = ax_c.imshow(curv_scaled, cmap='RdBu_r', vmin=-curv_max, vmax=curv_max,\n",
    "                           origin='lower', extent=data['extent_km'])\n",
    "        ax_c.set_title('Curvature (√ó1000 m‚Åª¬π)', fontsize=10)\n",
    "        style_axes(ax_c)\n",
    "        plt.colorbar(im_c, ax=ax_c, fraction=0.046, pad=0.04, label='Curvature')\n",
    "\n",
    "        # Red box for next level (all 4 panels)\n",
    "        if i < len(extracted) - 1:\n",
    "            nxt_win = extracted[i+1]['level']['window']\n",
    "            for ax in (ax_e, ax_s, ax_a, ax_c):\n",
    "                draw_next_extent(ax, nxt_win)\n",
    "\n",
    "    plt.suptitle(\n",
    "        f'üóª MASSIVE TERRAIN ANALYSIS - Multi-Scale Visualization üóª\\n'\n",
    "        f'Domain: {full_x_km:.0f}√ó{full_y_km:.0f} km  |  '\n",
    "        f'Pixels: {array_size[1]}√ó{array_size[0]}  |  Pixel: {pixel_size:.1f} m',\n",
    "        fontsize=16, fontweight='bold', y=0.98\n",
    "    )\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    return fig\n",
    "\n",
    "# === CALL ===\n",
    "print(\"üé¨ LAUNCHING SPECTACULAR MULTI-SCALE VISUALIZATION...\")\n",
    "viz_start = time.time()\n",
    "spectacular_fig = create_multiscale_visualization(\n",
    "    massive_terrain,\n",
    "    terrain_metrics['slope'],\n",
    "    terrain_metrics['aspect'],\n",
    "    terrain_metrics['curvature'],\n",
    "    ARRAY_SIZE,\n",
    "    PIXEL_SIZE\n",
    ")\n",
    "viz_time = time.time() - viz_start\n",
    "plt.show()\n",
    "print(f\"\\nüéâ VISUALIZATION COMPLETED in {viz_time:.2f}s! üéâ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-summary",
   "metadata": {},
   "source": [
    "## üìä ULTIMATE PERFORMANCE SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üèÜ\" + \"‚ïê\" * 60 + \"üèÜ\")\n",
    "print(\"    üî• DASK MASSIVE ARRAY PROCESSING - RESULTS üî•\")\n",
    "print(\"üèÜ\" + \"‚ïê\" * 60 + \"üèÜ\")\n",
    "\n",
    "# Calculate totals\n",
    "total_data_processed = 4 * np.prod(ARRAY_SIZE) * 8 / 1024**3  # 4 arrays √ó 8 bytes\n",
    "total_computation_time = terrain_time + viz_time\n",
    "final_memory = psutil.virtual_memory()\n",
    "\n",
    "print(f\"\\nüìè DATASET SCALE:\")\n",
    "print(f\"   üó∫Ô∏è  Geographic Coverage: {EXTENT_KM:.0f} km √ó {EXTENT_KM:.0f} km\")\n",
    "print(f\"   üî¢ Grid Resolution: {ARRAY_SIZE[0]:,} √ó {ARRAY_SIZE[1]:,} = {np.prod(ARRAY_SIZE):,} points\")\n",
    "print(f\"   üéØ Spatial Resolution: {PIXEL_SIZE} m/pixel\")\n",
    "print(f\"   üìä Total Points Analyzed: {np.prod(ARRAY_SIZE)/1e6:.0f} MILLION\")\n",
    "print(f\"   üíæ Raw Data Volume: ~{total_data_processed:.1f} GB\")\n",
    "\n",
    "print(f\"\\n‚ö° COMPUTATIONAL PERFORMANCE:\")\n",
    "print(f\"   ‚è±Ô∏è  Total Processing Time: {total_computation_time:.2f} seconds\")\n",
    "print(f\"   üöÄ Processing Throughput: {total_data_processed/total_computation_time:.2f} GB/s\")\n",
    "print(f\"   üè≠ Parallel Workers: {len(client.scheduler_info()['workers'])}\")\n",
    "print(f\"   üßµ Total Threads: {sum(w['nthreads'] for w in client.scheduler_info()['workers'].values())}\")\n",
    "print(f\"   üßÆ Points/Second: {np.prod(ARRAY_SIZE)/total_computation_time/1e6:.1f} Million/s\")\n",
    "\n",
    "print(f\"\\nüíæ MEMORY EFFICIENCY:\")\n",
    "print(f\"   üìà Peak RAM Usage: {final_memory.used/1024**3:.1f} GB / {final_memory.total/1024**3:.1f} GB\")\n",
    "print(f\"   üîÑ Memory Efficiency: {total_data_processed/(final_memory.used/1024**3):.1f}√ó data-to-RAM ratio\")\n",
    "print(f\"   ‚ôªÔ∏è  Out-of-Core: ‚úÖ Processed {total_data_processed:.1f} GB dataset\")\n",
    "\n",
    "print(f\"\\nüßÆ ANALYSIS ACHIEVEMENTS:\")\n",
    "print(f\"   üèîÔ∏è  Terrain Metrics: Elevation, Slope, Aspect, Curvature\")\n",
    "print(f\"   üìä Statistical Analysis: Min/Max/Mean/Std/Percentiles\")\n",
    "print(f\"   üéØ Classification: Steep areas, Convex/Concave regions\")\n",
    "print(f\"   üîç Multi-Scale Viz: 4 zoom levels (satellite ‚Üí hiking detail)\")\n",
    "print(f\"   üé® Advanced Rendering: Hillshade, color mapping, overlays\")\n",
    "\n",
    "print(f\"\\nüåü SCALE COMPARISON:\")\n",
    "print(f\"   üåç Area Equivalent: Larger than Belgium ({EXTENT_KM**2:.0f} km¬≤)\")\n",
    "print(f\"   üì± Storage: ~{total_data_processed/64:.1f}√ó iPhone 64GB capacity\")\n",
    "print(f\"   ‚ö° Speed: Analyzed entire country-scale terrain in {total_computation_time:.0f} seconds!\")\n",
    "\n",
    "print(f\"\\nüöÄ DASK SUPERPOWERS DEMONSTRATED:\")\n",
    "print(f\"   ‚úÖ Lazy Evaluation: Build complex computation graphs\")\n",
    "print(f\"   ‚úÖ Parallel Processing: Automatic multi-core utilization\") \n",
    "print(f\"   ‚úÖ Out-of-Core: Process datasets larger than memory\")\n",
    "print(f\"   ‚úÖ Chunked Operations: Efficient memory management\")\n",
    "print(f\"   ‚úÖ Boundary Handling: map_overlap for edge artifacts\")\n",
    "print(f\"   ‚úÖ Scalability: Same code ‚Üí laptop to supercomputer\")\n",
    "\n",
    "print(f\"\\n\" + \"üèÜ\" + \"‚ïê\" * 60 + \"üèÜ\")\n",
    "print(f\"   üéâ MISSION ACCOMPLISHED: MASSIVE SCALE MASTERED! üéâ\")\n",
    "print(f\"üèÜ\" + \"‚ïê\" * 60 + \"üèÜ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup",
   "metadata": {},
   "source": [
    "## üßπ Cleanup and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üßπ Cleaning up resources...\")\n",
    "\n",
    "# Close the Dask client\n",
    "client.close()\n",
    "print(\"‚úÖ Dask cluster shut down\")\n",
    "\n",
    "# Final memory check\n",
    "final_memory = psutil.virtual_memory()\n",
    "print(f\"üíæ Final memory usage: {final_memory.used/1024**3:.1f} GB\")\n",
    "\n",
    "print(f\"\\nüéì WHAT YOU LEARNED:\")\n",
    "print(f\"   üìö Dask fundamentals: arrays, chunks, lazy evaluation\")\n",
    "print(f\"   ‚ö° Parallel computing: multi-core processing\")\n",
    "print(f\"   üíæ Memory management: out-of-core operations\")\n",
    "print(f\"   üßÆ Advanced analytics: terrain analysis at scale\")\n",
    "print(f\"   üé® Data visualization: multi-scale presentations\")\n",
    "print(f\"   üèóÔ∏è  Boundary handling: map_overlap for correct gradients\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(f\"   üìà Scale up: Try even larger datasets\")\n",
    "print(f\"   üåê Distributed: Use Dask on multiple machines\")\n",
    "print(f\"   üß† ML integration: Combine with dask-ml for machine learning\")\n",
    "print(f\"   üìä Real data: Apply to satellite imagery, climate data\")\n",
    "print(f\"   ‚òÅÔ∏è  Cloud deployment: Use Dask on AWS, GCP, Azure\")\n",
    "\n",
    "print(f\"\\n‚ú® You just processed {np.prod(ARRAY_SIZE)/1e6:.0f} MILLION data points! ‚ú®\")\n",
    "print(f\"üèÜ Welcome to MASSIVE-SCALE computing with Dask! üèÜ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd25d3b-c58b-4a22-94f2-e6cec347341e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1665e2ce-e069-43b7-bc7b-71a75ba127b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
