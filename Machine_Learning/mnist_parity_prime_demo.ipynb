{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f571cea",
   "metadata": {},
   "source": [
    "\n",
    "# PyTorch in ~10 Minutes: **MNIST with a Twist (Parity & Prime)** — JupyterLab Friendly\n",
    "\n",
    "**What does this tuto do:**\n",
    "- Train a tiny **multi‑task Neural Network (CNN for convolutional -> perfect for 2D data like images or DEM)** on MNIST **subset** to predict:\n",
    "  - **Parity** — even vs odd\n",
    "  - **Prime** — prime vs non‑prime (2,3,5,7 are prime)\n",
    "- Then **draw your own digit** in a JupyterLab‑friendly canvas and get instant predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ▶️ Install (run once). ipycanvas gives a JupyterLab-friendly drawing widget.\n",
    "!pip -q install torch torchvision matplotlib scikit-learn pillow ipywidgets ipycanvas\n",
    "# In classic Notebook you may need: jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739b16db",
   "metadata": {},
   "source": [
    "\n",
    "## 0) **Hyperparameters to tweak** (only ones you should touch during the demo)\n",
    "\n",
    "Keep everything else stable. These are safe to play with live:\n",
    "- `TRAIN_SIZE`, `VAL_SIZE` — subset sizes (speed vs accuracy) TRAIN = directly feed to the NN for calculating the weights, VAL = Validation of the NN performances (independant dataset)\n",
    "- `BATCH_SIZE` — memory vs gradient noise\n",
    "- `EPOCHS` — more epochs → better accuracy (to a point) -> Number of training iterations\n",
    "- `LR` — learning rate; too high explodes, too low crawls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5448e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 🔧 Hyperparameters\n",
    "SEED       = 42 # Random seed for deterministic random number generations\n",
    "# How many images are used to TRAIN the model\n",
    "TRAIN_SIZE = 15000   # out of 60k in the OG dataset - images are small so it is not that big - like 70 Mb\n",
    "# How many images are used to VALIDATE the model\n",
    "VAL_SIZE   = 3000    # out of 10k\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS     = 20       # still quick\n",
    "LR         = 1e-3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005e4a1",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Imports & basic setup\n",
    "\n",
    "This is boilerplate: libraries, device (CPU/GPU), and reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be7462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time, random, math, io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Widgets for the JupyterLab drawing pad\n",
    "from ipywidgets import VBox, HBox, Button, Label, HTML\n",
    "from ipycanvas import Canvas\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cac741",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Download & prepare dataset **(what's happening here)**\n",
    "\n",
    "- We download **MNIST** (28×28 grayscale). Each sample is a **digit** (0–9).\n",
    "- We **relabel** each digit into **two targets**:\n",
    "  - `parity` = 0 if even, 1 if odd\n",
    "  - `prime`  = 1 if digit in {2,3,5,7} else 0  \n",
    "- We make **train/validation** splits:\n",
    "  - **Train**: used to update weights.\n",
    "  - **Validation**: never used to update, only to **judge generalization**.\n",
    "- For speed, we use **small subsets**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb307725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal transform: tensors in [0,1]\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_full = datasets.MNIST(root='./data', train=True,  download=True, transform=transform)\n",
    "test_full  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Subsample for speed\n",
    "rng = np.random.default_rng(SEED)\n",
    "idx_train = rng.choice(len(train_full), size=TRAIN_SIZE, replace=False)\n",
    "idx_val   = rng.choice(len(test_full),  size=VAL_SIZE,   replace=False)\n",
    "train_ds = Subset(train_full, idx_train)\n",
    "val_ds   = Subset(test_full,  idx_val)\n",
    "\n",
    "def make_labels(y):\n",
    "    y = int(y)\n",
    "    parity = y % 2              # 0 even, 1 odd\n",
    "    prime = 1 if y in {2,3,5,7} else 0\n",
    "    return parity, prime\n",
    "\n",
    "class TwistWrapper(Dataset):\n",
    "    \"\"\"Wrap MNIST to emit (image, parity, prime, digit)\"\"\"\n",
    "    def __init__(self, base): self.base = base\n",
    "    def __len__(self): return len(self.base)\n",
    "    def __getitem__(self, i):\n",
    "        x, y_digit = self.base[i]\n",
    "        parity, prime = make_labels(y_digit)\n",
    "        return x, torch.tensor(parity), torch.tensor(prime), y_digit\n",
    "\n",
    "train_wrap = TwistWrapper(train_ds)\n",
    "val_wrap   = TwistWrapper(val_ds)\n",
    "\n",
    "train_loader = DataLoader(train_wrap, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_wrap,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_wrap), len(val_wrap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff2bdc",
   "metadata": {},
   "source": [
    "\n",
    "### Quick peek at a few samples\n",
    "\n",
    "Always sanity‑check: are shapes and labels what we expect?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xb, ypar, yprm, ydig = next(iter(train_loader))\n",
    "fig, axes = plt.subplots(1, 6, figsize=(9,2))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(xb[i,0].numpy(), cmap='gray')\n",
    "    ax.set_title(f\"d={int(ydig[i])}\\npar={int(ypar[i])},prm={int(yprm[i])}\", fontsize=9)\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0152be50",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Model architecture **(what & why)**\n",
    "\n",
    "- A tiny **CNN encoder** extracts features from 28×28 images.\n",
    "- Two small **heads** (linear layers) make task‑specific predictions:\n",
    "  - **Parity head** → 2 classes (even/odd)\n",
    "  - **Prime head**  → 2 classes (non‑prime/prime)\n",
    "- Loss = **CE(parity)** + **CE(prime)** so both tasks learn together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491993a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TinyTwistNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),               # 28->14\n",
    "            nn.Conv2d(16, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),               # 14->7\n",
    "        )\n",
    "        self.head_parity = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        self.head_prime = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.head_parity(z), self.head_prime(z)\n",
    "\n",
    "model = TinyTwistNet().to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "print('Total parameters:', sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23714716",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Training loop **(how it learns)**\n",
    "\n",
    "For each mini‑batch:\n",
    "1. Forward pass → **two logits** (parity, prime)  \n",
    "2. Compute two **cross‑entropy** losses → **sum**  \n",
    "3. Backprop & optimizer step  \n",
    "We track **accuracy** for both tasks on **train** and **validation**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0864c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def step_acc(logits, y):\n",
    "    return (logits.argmax(1) == y).float().mean().item()\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    total, n = 0.0, 0\n",
    "    acc_p, acc_q = 0.0, 0.0\n",
    "    for xb, ypar, yprm, _ in loader:\n",
    "        xb, ypar, yprm = xb.to(device), ypar.to(device), yprm.to(device)\n",
    "        with torch.set_grad_enabled(train):\n",
    "            lp, lq = model(xb)\n",
    "            loss = crit(lp, ypar) + crit(lq, yprm)\n",
    "            if train:\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "        bsz = xb.size(0); n += bsz; total += loss.item()*bsz\n",
    "        acc_p += step_acc(lp, ypar)*bsz\n",
    "        acc_q += step_acc(lq, yprm)*bsz\n",
    "    return total/n, acc_p/n, acc_q/n\n",
    "\n",
    "history = {'train':[], 'val':[]}\n",
    "t0 = time.time()\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_p, tr_q = run_epoch(train_loader, True)\n",
    "    va_loss, va_p, va_q = run_epoch(val_loader, False)\n",
    "    history['train'].append((tr_loss, tr_p, tr_q))\n",
    "    history['val'].append((va_loss, va_p, va_q))\n",
    "    print(f\"Epoch {epoch:02d} | loss T/V: {tr_loss:.3f}/{va_loss:.3f} | acc parity T/V: {tr_p:.3f}/{va_p:.3f} | acc prime T/V: {tr_q:.3f}/{va_q:.3f}\")\n",
    "print(f\"Training time: {time.time()-t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ac671",
   "metadata": {},
   "source": [
    "\n",
    "### 5) Quick curves (did it converge?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c8660",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tr_p = [x[1] for x in history['train']]; va_p = [x[1] for x in history['val']]\n",
    "tr_q = [x[2] for x in history['train']]; va_q = [x[2] for x in history['val']]\n",
    "\n",
    "plt.figure(figsize=(6,3.5))\n",
    "plt.plot(tr_p, label='train parity'); plt.plot(va_p, label='val parity')\n",
    "plt.xlabel('epoch'); plt.ylabel('accuracy'); plt.title('Parity accuracy'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,3.5))\n",
    "plt.plot(tr_q, label='train prime'); plt.plot(va_q, label='val prime')\n",
    "plt.xlabel('epoch'); plt.ylabel('accuracy'); plt.title('Prime accuracy'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6eae33",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Validation report (numbers to show)\n",
    "\n",
    "We compute a compact **classification report** for each task on the validation subset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "par_true, par_pred, prm_true, prm_pred = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for xb, ypar, yprm, _ in val_loader:\n",
    "        lp, lq = model(xb.to(device))\n",
    "        par_true.append(ypar); prm_true.append(yprm)\n",
    "        par_pred.append(lp.cpu().argmax(1)); prm_pred.append(lq.cpu().argmax(1))\n",
    "par_true = torch.cat(par_true).numpy(); par_pred = torch.cat(par_pred).numpy()\n",
    "prm_true = torch.cat(prm_true).numpy(); prm_pred = torch.cat(prm_pred).numpy()\n",
    "\n",
    "print(\"=== Parity (0=even, 1=odd) ===\\n\", classification_report(par_true, par_pred, digits=4))\n",
    "print(\"=== Prime (0=non‑prime, 1=prime) ===\\n\", classification_report(prm_true, prm_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d5b13",
   "metadata": {},
   "source": [
    "\n",
    "## 7) **Draw your own digit** (JupyterLab‑friendly)\n",
    "\n",
    "Use the canvas below. Click‑drag to draw, **Clear** to reset, then **Predict**.  \n",
    "We downsample to 28×28 like MNIST and run the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cca746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import VBox, HBox, Button, HTML, IntSlider, Layout\n",
    "from ipycanvas import Canvas\n",
    "import numpy as np, torch\n",
    "from PIL import Image\n",
    "\n",
    "# --- Canvas config (internal vs displayed size) ---\n",
    "nx, ny = 128, 128            # internal resolution used for downsampling to 28×28\n",
    "display_px = 220             # visual size on screen (shrink/grow here)\n",
    "\n",
    "canvas = Canvas(\n",
    "    width=nx, height=ny, sync_image_data=True,\n",
    "    layout=Layout(width=f'{display_px}px', height=f'{display_px}px', border='1px solid #ccc')\n",
    ")\n",
    "canvas.fill_style = 'white'; canvas.fill_rect(0,0,nx,ny)\n",
    "canvas.stroke_style = 'black'\n",
    "\n",
    "brush = IntSlider(value=10, min=3, max=28, step=1, description='Brush', continuous_update=True)\n",
    "canvas.line_width = brush.value\n",
    "\n",
    "def _set_brush(change):\n",
    "    canvas.line_width = change['new']\n",
    "brush.observe(_set_brush, names='value')\n",
    "\n",
    "# --- Drawing handlers ---\n",
    "is_drawing, last = False, None\n",
    "def handle_mousedown(x, y):\n",
    "    global is_drawing, last\n",
    "    is_drawing, last = True, (x, y)\n",
    "\n",
    "def handle_mousemove(x, y):\n",
    "    global is_drawing, last\n",
    "    if not is_drawing: return\n",
    "    lx, ly = last if last else (x, y)\n",
    "    canvas.begin_path(); canvas.move_to(lx, ly); canvas.line_to(x, y); canvas.stroke()\n",
    "    last = (x, y)\n",
    "\n",
    "def handle_mouseup(x, y):\n",
    "    global is_drawing, last\n",
    "    is_drawing, last = False, None\n",
    "\n",
    "canvas.on_mouse_down(handle_mousedown)\n",
    "canvas.on_mouse_move(handle_mousemove)\n",
    "canvas.on_mouse_up(handle_mouseup)\n",
    "\n",
    "# --- Buttons & helpers ---\n",
    "btn_clear = Button(description='Clear', button_style='warning')\n",
    "btn_pred  = Button(description='Predict', button_style='success')\n",
    "out_lbl   = HTML(value='Draw a digit, then click <b>Predict</b>.')\n",
    "\n",
    "def clear_canvas(_):\n",
    "    canvas.fill_style = 'white'; canvas.fill_rect(0,0,nx,ny)\n",
    "btn_clear.on_click(clear_canvas)\n",
    "\n",
    "def preprocess_canvas_to_tensor():\n",
    "    data = np.asarray(canvas.get_image_data())     # (H,W,4) uint8\n",
    "    rgb  = data[..., :3].astype(np.float32)\n",
    "    gray = (0.299*rgb[...,0] + 0.587*rgb[...,1] + 0.114*rgb[...,2]) / 255.0\n",
    "    img  = Image.fromarray((gray*255).astype(np.uint8)).resize((28,28), Image.BILINEAR)\n",
    "    arr  = np.asarray(img).astype(np.float32)/255.0\n",
    "    x28  = 1.0 - arr                               # white digit on black like MNIST\n",
    "    return torch.from_numpy(x28[None, None, :, :]).float().to(device)\n",
    "\n",
    "def predict_canvas(_):\n",
    "    with torch.no_grad():\n",
    "        x = preprocess_canvas_to_tensor()\n",
    "        lp, lq = model(x)\n",
    "        par = lp.argmax(1).item(); prm = lq.argmax(1).item()\n",
    "    out_lbl.value = f\"<b>Prediction:</b> Parity = {'odd' if par==1 else 'even'} | Prime = {'prime' if prm==1 else 'non-prime'}\"\n",
    "\n",
    "btn_pred.on_click(predict_canvas)\n",
    "\n",
    "VBox([canvas, HBox([btn_clear, btn_pred, brush]), out_lbl])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe5892",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Load a 28×28 PNG\n",
    "\n",
    "If you prefer an image file: must be **28×28 grayscale**; white digit on black is expected (we invert if needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef194238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_image(path):\n",
    "    img = Image.open(path).convert('L').resize((28,28))\n",
    "    arr = np.array(img).astype(np.float32)/255.0\n",
    "    x28 = 1.0 - arr\n",
    "    x_tensor = torch.from_numpy(x28[None,None,:,:]).float().to(device)\n",
    "    with torch.no_grad():\n",
    "        lp, lq = model(x_tensor)\n",
    "        pred_par = lp.argmax(1).item()\n",
    "        pred_prm = lq.argmax(1).item()\n",
    "    print(f\"Parity: {'odd' if pred_par==1 else 'even'} | Prime: {'prime' if pred_prm==1 else 'non‑prime'}\")\n",
    "\n",
    "# predict_image('my_digit_28x28.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02f1cb",
   "metadata": {},
   "source": [
    "\n",
    "## 8) Wrap‑up (talk track)\n",
    "\n",
    "- **This cell**: model architecture (shared encoder + two heads).  \n",
    "- **This cell**: dataset download + relabel to parity/prime.  \n",
    "- **This cell**: hyperparameters are isolated so you can **safely tweak** them live.  \n",
    "- **Training cell**: shows the **whole learning loop** is a few lines.  \n",
    "- **Widgets cell**: audience **draws new data** → instant parity/prime.  \n",
    "\n",
    "**Extensions (if time)**: add third head (multiple of 3), log confusion matrices, or export to TorchScript.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
