{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro_cell",
   "metadata": {},
   "source": [
    "# JAX Performance Demonstration\n",
    "\n",
    "JAX is a high-performance machine learning library that provides:\n",
    "\n",
    "- **Just-in-time (JIT) compilation** using XLA compiler\n",
    "- **Automatic vectorization** for SIMD operations\n",
    "- **GPU/TPU acceleration** with the same code\n",
    "- **Automatic differentiation** for gradients\n",
    "- **Function transformations** like vmap for vectorization\n",
    "\n",
    "This notebook demonstrates JAX's performance advantages over NumPy for numerical computations through JIT compilation and vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "from jax import jit, vmap\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check JAX backend and devices\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"Available devices: {jax.devices()}\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_explanation",
   "metadata": {},
   "source": [
    "## Array Creation and Basic Operations\n",
    "\n",
    "JAX arrays (DeviceArrays) live on accelerators by default. We'll compare array operations between NumPy and JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic_operations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large arrays for performance testing\n",
    "n = 10_000_000  # 10 million elements\n",
    "print(f\"Array size: {n:,} elements\")\n",
    "\n",
    "# Create NumPy arrays first with consistent seed\n",
    "np.random.seed(42)\n",
    "np_a = np.random.randn(n).astype(np.float32)\n",
    "np_b = np.random.randn(n).astype(np.float32)\n",
    "np_c = np.random.randn(n).astype(np.float32)\n",
    "\n",
    "# Create JAX arrays using the SAME data for fair comparison\n",
    "jax_a = jnp.array(np_a)  # Same data as NumPy\n",
    "jax_b = jnp.array(np_b)  # Same data as NumPy  \n",
    "jax_c = jnp.array(np_c)  # Same data as NumPy\n",
    "\n",
    "print(f\"NumPy array type: {type(np_a)}\")\n",
    "print(f\"JAX array type: {type(jax_a)}\")\n",
    "print(f\"JAX array device: {jax_a.device}\")\n",
    "\n",
    "# Verify arrays contain identical data\n",
    "print(f\"Arrays contain same data: {np.array_equal(np_a, jax_a)}\")\n",
    "print(f\"Data precision: {np_a.dtype} (NumPy) vs {jax_a.dtype} (JAX)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jit_explanation",
   "metadata": {},
   "source": [
    "## JIT Compilation Benefits\n",
    "\n",
    "JAX's key advantage comes from JIT compilation, but realistic expectations are important:\n",
    "\n",
    "**Expected performance gains:**\n",
    "- **2-10x speedup** for compute-heavy operations\n",
    "- **Larger gains** on GPU vs CPU\n",
    "- **Best performance** after warmup (compilation overhead)\n",
    "- **Diminishing returns** for simple operations\n",
    "\n",
    "**Important caveats:**\n",
    "- First call includes compilation time (slower)\n",
    "- GPU kernels have launch overhead\n",
    "- Memory transfer costs (CPU ↔ GPU) \n",
    "- float32 vs float64 precision differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jit_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define computation functions\n",
    "def numpy_computation(a, b, c):\n",
    "    \"\"\"\n",
    "    Standard NumPy computation - interpreted Python with vectorized operations.\n",
    "    Each operation is dispatched individually to optimized C code.\n",
    "    \"\"\"\n",
    "    # Complex mathematical expression\n",
    "    result = np.tanh(a) * np.exp(-b**2) + np.sin(c) * np.cos(a)\n",
    "    result = result + np.sqrt(np.abs(a * b)) - np.log1p(np.abs(c))\n",
    "    return result\n",
    "\n",
    "def jax_computation(a, b, c):\n",
    "    \"\"\"\n",
    "    JAX computation - can be JIT compiled for optimization.\n",
    "    Operations are fused and optimized by XLA compiler.\n",
    "    \"\"\"\n",
    "    # Same mathematical expression as NumPy version\n",
    "    result = jnp.tanh(a) * jnp.exp(-b**2) + jnp.sin(c) * jnp.cos(a)\n",
    "    result = result + jnp.sqrt(jnp.abs(a * b)) - jnp.log1p(jnp.abs(c))\n",
    "    return result\n",
    "\n",
    "# Create JIT-compiled version\n",
    "jax_computation_jit = jit(jax_computation)\n",
    "\n",
    "print(\"Functions defined - ready for benchmarking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROPER BENCHMARKING with synchronization and warmup\n",
    "\n",
    "print(\"Starting proper benchmark with identical input data...\")\n",
    "\n",
    "# Warmup runs for JIT compilation\n",
    "print(\"Warming up JIT compilation...\")\n",
    "for _ in range(3):\n",
    "    _ = jax_computation_jit(jax_a, jax_b, jax_c).block_until_ready()\n",
    "\n",
    "# Multiple runs for stable timing\n",
    "n_runs = 5\n",
    "numpy_times = []\n",
    "jax_times = []\n",
    "jax_jit_times = []\n",
    "\n",
    "print(f\"Running {n_runs} iterations for stable timing...\")\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # Benchmark NumPy\n",
    "    start = time.perf_counter()\n",
    "    numpy_result = numpy_computation(np_a, np_b, np_c)\n",
    "    numpy_time = time.perf_counter() - start\n",
    "    numpy_times.append(numpy_time)\n",
    "    \n",
    "    # Benchmark JAX without JIT\n",
    "    start = time.perf_counter()\n",
    "    jax_result = jax_computation(jax_a, jax_b, jax_c)\n",
    "    jax_result.block_until_ready()  # Critical: wait for GPU computation\n",
    "    jax_time = time.perf_counter() - start\n",
    "    jax_times.append(jax_time)\n",
    "    \n",
    "    # Benchmark JAX with JIT (already compiled)\n",
    "    start = time.perf_counter()\n",
    "    jax_jit_result = jax_computation_jit(jax_a, jax_b, jax_c)\n",
    "    jax_jit_result.block_until_ready()  # Critical: wait for GPU computation  \n",
    "    jax_jit_time = time.perf_counter() - start\n",
    "    jax_jit_times.append(jax_jit_time)\n",
    "\n",
    "# Calculate statistics\n",
    "numpy_avg = np.mean(numpy_times)\n",
    "numpy_std = np.std(numpy_times)\n",
    "jax_avg = np.mean(jax_times)\n",
    "jax_std = np.std(jax_times)\n",
    "jax_jit_avg = np.mean(jax_jit_times)\n",
    "jax_jit_std = np.std(jax_jit_times)\n",
    "\n",
    "print(f\"\\\\nPerformance Results (average ± std over {n_runs} runs):\")\n",
    "print(f\"NumPy time:           {numpy_avg:.4f}s ± {numpy_std:.4f}s\")\n",
    "print(f\"JAX time (no JIT):    {jax_avg:.4f}s ± {jax_std:.4f}s\") \n",
    "print(f\"JAX time (JIT):       {jax_jit_avg:.4f}s ± {jax_jit_std:.4f}s\")\n",
    "\n",
    "print(f\"\\\\nRealistic Speedup Analysis:\")\n",
    "speedup_jax = numpy_avg / jax_avg if jax_avg > 0 else 0\n",
    "speedup_jit = numpy_avg / jax_jit_avg if jax_jit_avg > 0 else 0\n",
    "jit_improvement = jax_avg / jax_jit_avg if jax_jit_avg > 0 else 0\n",
    "\n",
    "print(f\"JAX vs NumPy (no JIT):  {speedup_jax:.1f}x\")\n",
    "print(f\"JAX vs NumPy (JIT):     {speedup_jit:.1f}x\") \n",
    "print(f\"JIT improvement:        {jit_improvement:.1f}x\")\n",
    "\n",
    "# Verify numerical accuracy with proper tolerances\n",
    "max_diff = np.max(np.abs(numpy_result - np.array(jax_jit_result)))\n",
    "print(f\"\\\\nNumerical verification (using IDENTICAL input data):\")\n",
    "print(f\"Max difference: {max_diff:.2e}\")\n",
    "results_match = np.allclose(numpy_result, jax_jit_result, rtol=1e-6, atol=1e-7)\n",
    "print(f\"Results match: {results_match}\")\n",
    "\n",
    "if not results_match:\n",
    "    print(f\"Float32 machine epsilon: {np.finfo(np.float32).eps:.2e}\")\n",
    "    print(f\"Relative error: {max_diff/np.max(np.abs(numpy_result)):.2e}\")\n",
    "\n",
    "print(f\"\\\\nNote: Using identical input data ensures numerical accuracy verification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48m5kesr0at",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETAILED NUMERICAL VERIFICATION - Let's debug the differences\n",
    "\n",
    "print(\"=== DEBUGGING NUMERICAL DIFFERENCES ===\")\n",
    "\n",
    "# First, let's check if the issue is data type conversion\n",
    "print(\"\\\\n1. Data type analysis:\")\n",
    "print(f\"NumPy array dtype: {np_a.dtype}\")\n",
    "print(f\"JAX array dtype: {jax_a.dtype}\")\n",
    "\n",
    "# Let's use a small subset for detailed analysis\n",
    "n_small = 1000\n",
    "np_small_a = np_a[:n_small]\n",
    "np_small_b = np_b[:n_small] \n",
    "np_small_c = np_c[:n_small]\n",
    "jax_small_a = jax_a[:n_small]\n",
    "jax_small_b = jax_b[:n_small]\n",
    "jax_small_c = jax_c[:n_small]\n",
    "\n",
    "print(f\"\\\\n2. Small subset verification (first {n_small} elements):\")\n",
    "\n",
    "# Step-by-step computation verification\n",
    "print(\"\\\\nStep-by-step verification:\")\n",
    "\n",
    "# Step 1: tanh(a) * exp(-b^2)\n",
    "np_step1 = np.tanh(np_small_a) * np.exp(-np_small_b**2)\n",
    "jax_step1 = jnp.tanh(jax_small_a) * jnp.exp(-jax_small_b**2)\n",
    "diff1 = np.max(np.abs(np_step1 - np.array(jax_step1)))\n",
    "print(f\"Step 1 max diff: {diff1:.2e}\")\n",
    "\n",
    "# Step 2: sin(c) * cos(a)  \n",
    "np_step2 = np.sin(np_small_c) * np.cos(np_small_a)\n",
    "jax_step2 = jnp.sin(jax_small_c) * jnp.cos(jax_small_a)\n",
    "diff2 = np.max(np.abs(np_step2 - np.array(jax_step2)))\n",
    "print(f\"Step 2 max diff: {diff2:.2e}\")\n",
    "\n",
    "# Step 3: sqrt(abs(a * b))\n",
    "np_step3 = np.sqrt(np.abs(np_small_a * np_small_b))\n",
    "jax_step3 = jnp.sqrt(jnp.abs(jax_small_a * jax_small_b))\n",
    "diff3 = np.max(np.abs(np_step3 - np.array(jax_step3)))\n",
    "print(f\"Step 3 max diff: {diff3:.2e}\")\n",
    "\n",
    "# Step 4: log1p(abs(c))\n",
    "np_step4 = np.log1p(np.abs(np_small_c))\n",
    "jax_step4 = jnp.log1p(jnp.abs(jax_small_c))\n",
    "diff4 = np.max(np.abs(np_step4 - np.array(jax_step4)))\n",
    "print(f\"Step 4 max diff: {diff4:.2e}\")\n",
    "\n",
    "# Full computation on small subset\n",
    "np_small_result = numpy_computation(np_small_a, np_small_b, np_small_c)\n",
    "jax_small_result = jax_computation(jax_small_a, jax_small_b, jax_small_c)\n",
    "\n",
    "diff_total = np.abs(np_small_result - np.array(jax_small_result))\n",
    "print(f\"\\\\nTotal computation differences:\")\n",
    "print(f\"Max diff: {np.max(diff_total):.2e}\")\n",
    "print(f\"Mean diff: {np.mean(diff_total):.2e}\")\n",
    "print(f\"Std diff: {np.std(diff_total):.2e}\")\n",
    "\n",
    "# Check if differences are reasonable for float32\n",
    "expected_eps = np.finfo(np.float32).eps * 10  # Allow 10x machine epsilon\n",
    "print(f\"\\\\nFloat32 machine epsilon: {np.finfo(np.float32).eps:.2e}\")\n",
    "print(f\"Expected tolerance (10x eps): {expected_eps:.2e}\")\n",
    "print(f\"All differences within tolerance: {np.all(diff_total < expected_eps)}\")\n",
    "\n",
    "# Look at worst cases\n",
    "worst_indices = np.argsort(diff_total)[-5:]\n",
    "print(f\"\\\\nWorst 5 differences:\")\n",
    "for i, idx in enumerate(worst_indices):\n",
    "    print(f\"{i+1}. Index {idx}: np={np_small_result[idx]:.6f}, jax={jax_small_result[idx]:.6f}, diff={diff_total[idx]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectorization_explanation",
   "metadata": {},
   "source": [
    "## Automatic Vectorization with vmap\n",
    "\n",
    "JAX's `vmap` transformation automatically vectorizes functions, eliminating explicit loops and providing better performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vmap_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch data for vectorization demo\n",
    "batch_size = 1000\n",
    "vector_size = 10000\n",
    "\n",
    "# Create NumPy batch data first\n",
    "np.random.seed(123)  # Different seed for batch demo\n",
    "np_batch = np.random.randn(batch_size, vector_size).astype(np.float32)\n",
    "np_weights = np.random.randn(vector_size).astype(np.float32)\n",
    "\n",
    "# Create JAX batch data using SAME data\n",
    "jax_batch = jnp.array(np_batch)\n",
    "jax_weights = jnp.array(np_weights)\n",
    "\n",
    "def single_computation(x, w):\n",
    "    \"\"\"Computation on a single vector\"\"\"\n",
    "    return jnp.tanh(jnp.dot(x, w)) + jnp.sum(jnp.sin(x))\n",
    "\n",
    "# NumPy approach: explicit loop\n",
    "def numpy_batch_computation(batch, weights):\n",
    "    \"\"\"Process batch with explicit Python loop\"\"\"\n",
    "    results = []\n",
    "    for i in range(batch.shape[0]):\n",
    "        x = batch[i]\n",
    "        result = np.tanh(np.dot(x, weights)) + np.sum(np.sin(x))\n",
    "        results.append(result)\n",
    "    return np.array(results)\n",
    "\n",
    "# JAX approach: vmap for automatic vectorization\n",
    "jax_batch_computation = vmap(single_computation, in_axes=(0, None))\n",
    "jax_batch_computation_jit = jit(jax_batch_computation)\n",
    "\n",
    "print(f\"Batch processing: {batch_size} vectors of size {vector_size}\")\n",
    "print(f\"Total elements: {batch_size * vector_size:,}\")\n",
    "print(f\"Using identical input data: {np.array_equal(np_batch, jax_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vmap_benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROPER VMAP BENCHMARKING with synchronization and warmup\n",
    "\n",
    "print(\"Proper vmap benchmarking with identical input data...\")\n",
    "\n",
    "# Warmup JIT compilation\n",
    "print(\"Warming up vmap JIT compilation...\")\n",
    "for _ in range(3):\n",
    "    _ = jax_batch_computation_jit(jax_batch, jax_weights).block_until_ready()\n",
    "\n",
    "# Multiple runs for reliable timing\n",
    "n_runs = 5\n",
    "numpy_batch_times = []\n",
    "jax_vmap_times = []\n",
    "jax_vmap_jit_times = []\n",
    "\n",
    "print(f\"Running {n_runs} iterations...\")\n",
    "\n",
    "for i in range(n_runs):\n",
    "    # NumPy with explicit loop\n",
    "    start = time.perf_counter()\n",
    "    numpy_batch_result = numpy_batch_computation(np_batch, np_weights)\n",
    "    numpy_batch_time = time.perf_counter() - start\n",
    "    numpy_batch_times.append(numpy_batch_time)\n",
    "    \n",
    "    # JAX with vmap (no JIT)  \n",
    "    start = time.perf_counter()\n",
    "    jax_batch_result = jax_batch_computation(jax_batch, jax_weights)\n",
    "    jax_batch_result.block_until_ready()  # Critical: wait for computation\n",
    "    jax_vmap_time = time.perf_counter() - start\n",
    "    jax_vmap_times.append(jax_vmap_time)\n",
    "    \n",
    "    # JAX with vmap + JIT (already compiled)\n",
    "    start = time.perf_counter()\n",
    "    jax_batch_jit_result = jax_batch_computation_jit(jax_batch, jax_weights)\n",
    "    jax_batch_jit_result.block_until_ready()  # Critical: wait for computation\n",
    "    jax_vmap_jit_time = time.perf_counter() - start\n",
    "    jax_vmap_jit_times.append(jax_vmap_jit_time)\n",
    "\n",
    "# Calculate averages and standard deviations\n",
    "numpy_avg = np.mean(numpy_batch_times)\n",
    "numpy_std = np.std(numpy_batch_times)\n",
    "vmap_avg = np.mean(jax_vmap_times)\n",
    "vmap_std = np.std(jax_vmap_times)\n",
    "vmap_jit_avg = np.mean(jax_vmap_jit_times)\n",
    "vmap_jit_std = np.std(jax_vmap_jit_times)\n",
    "\n",
    "print(f\"\\\\nBatch Processing Results (average ± std over {n_runs} runs):\")\n",
    "print(f\"NumPy (explicit loop):    {numpy_avg:.4f}s ± {numpy_std:.4f}s\")\n",
    "print(f\"JAX vmap (no JIT):        {vmap_avg:.4f}s ± {vmap_std:.4f}s\")\n",
    "print(f\"JAX vmap (JIT):           {vmap_jit_avg:.4f}s ± {vmap_jit_std:.4f}s\")\n",
    "\n",
    "print(f\"\\\\nRealistic Vectorization Speedup:\")\n",
    "speedup_vmap = numpy_avg / vmap_avg if vmap_avg > 0 else 0\n",
    "speedup_vmap_jit = numpy_avg / vmap_jit_avg if vmap_jit_avg > 0 else 0\n",
    "\n",
    "print(f\"vmap vs NumPy loop:       {speedup_vmap:.1f}x\")\n",
    "print(f\"vmap+JIT vs NumPy loop:   {speedup_vmap_jit:.1f}x\")\n",
    "\n",
    "# Verify numerical accuracy with IDENTICAL input data\n",
    "print(f\"\\\\nNumerical verification (using IDENTICAL input data):\")\n",
    "print(f\"Results shape: {jax_batch_jit_result.shape}\")\n",
    "\n",
    "max_diff = np.max(np.abs(numpy_batch_result - np.array(jax_batch_jit_result)))\n",
    "print(f\"Max difference: {max_diff:.2e}\")\n",
    "\n",
    "results_match = np.allclose(numpy_batch_result, jax_batch_jit_result, rtol=1e-6, atol=1e-7)\n",
    "print(f\"Results match: {results_match}\")\n",
    "\n",
    "if not results_match:\n",
    "    print(f\"Mean difference: {np.mean(np.abs(numpy_batch_result - np.array(jax_batch_jit_result))):.2e}\")\n",
    "    print(f\"Float32 machine epsilon: {np.finfo(np.float32).eps:.2e}\")\n",
    "\n",
    "print(f\"\\\\nNote: Identical inputs ensure accurate numerical comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "memory_analysis",
   "metadata": {},
   "source": [
    "## Memory Efficiency and Device Management\n",
    "\n",
    "JAX manages memory efficiently and can work seamlessly across different devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "memory_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficiency demonstration\n",
    "print(\"Memory and Device Analysis:\")\n",
    "print(f\"NumPy array location: CPU memory\")\n",
    "print(f\"JAX array location: {jax_a.device}\")  # Fixed: device is a property\n",
    "\n",
    "# Show memory usage patterns\n",
    "small_size = 1000\n",
    "np_small = np.random.randn(small_size)\n",
    "jax_small = jax.random.normal(key, (small_size,))\n",
    "\n",
    "print(f\"\\nSmall array sizes: {small_size} elements\")\n",
    "print(f\"NumPy itemsize: {np_small.itemsize} bytes\")\n",
    "print(f\"JAX itemsize: {jax_small.itemsize} bytes\")\n",
    "\n",
    "# Device transfer overhead\n",
    "print(f\"\\nDevice transfer (if applicable):\")\n",
    "start = time.perf_counter()\n",
    "jax_to_numpy = np.array(jax_small)  # Device to CPU\n",
    "transfer_time = time.perf_counter() - start\n",
    "print(f\"JAX to NumPy transfer: {transfer_time:.6f}s\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "numpy_to_jax = jnp.array(np_small)  # CPU to device\n",
    "transfer_time2 = time.perf_counter() - start\n",
    "print(f\"NumPy to JAX transfer: {transfer_time2:.6f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical_guidelines",
   "metadata": {},
   "source": [
    "## Practical Usage Guidelines\n",
    "\n",
    "**When to use JAX:**\n",
    "- Computationally intensive numerical operations\n",
    "- Machine learning and scientific computing\n",
    "- When you need automatic differentiation\n",
    "- Batch processing with consistent operations\n",
    "- GPU/TPU acceleration available\n",
    "\n",
    "**When to stick with NumPy:**\n",
    "- Simple, one-off computations\n",
    "- Extensive use of NumPy ecosystem (scipy, pandas)\n",
    "- Frequent interaction with Python objects\n",
    "- When compilation overhead outweighs benefits\n",
    "- I/O heavy workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compilation_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of when JIT compilation pays off\n",
    "sizes = [1000, 10000, 100000, 1000000]\n",
    "compile_times = []\n",
    "execution_speedups = []\n",
    "\n",
    "print(\"JIT Compilation Payoff Analysis:\")\n",
    "print(f\"{'Size':<10} {'Compile(ms)':<12} {'Speedup':<10} {'Break-even':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "def test_function(x):\n",
    "    return jnp.sin(x) * jnp.cos(x) + jnp.exp(-x**2)\n",
    "\n",
    "test_function_jit = jit(test_function)\n",
    "\n",
    "for size in sizes:\n",
    "    # Create test array\n",
    "    test_arr = jax.random.normal(key, (size,))\n",
    "    \n",
    "    # Time regular JAX\n",
    "    start = time.perf_counter()\n",
    "    regular_result = test_function(test_arr)\n",
    "    regular_result.block_until_ready()\n",
    "    regular_time = time.perf_counter() - start\n",
    "    \n",
    "    # Time JIT compilation + execution\n",
    "    start = time.perf_counter()\n",
    "    jit_result = test_function_jit(test_arr)\n",
    "    jit_result.block_until_ready()\n",
    "    compile_time = time.perf_counter() - start\n",
    "    \n",
    "    # Time JIT execution only\n",
    "    start = time.perf_counter()\n",
    "    jit_result2 = test_function_jit(test_arr)\n",
    "    jit_result2.block_until_ready()\n",
    "    execution_time = time.perf_counter() - start\n",
    "    \n",
    "    speedup = regular_time / execution_time\n",
    "    compilation_overhead = compile_time - execution_time\n",
    "    break_even_calls = compilation_overhead / (regular_time - execution_time) if execution_time < regular_time else float('inf')\n",
    "    \n",
    "    compile_times.append(compilation_overhead * 1000)  # Convert to ms\n",
    "    execution_speedups.append(speedup)\n",
    "    \n",
    "    print(f\"{size:<10} {compilation_overhead*1000:<12.2f} {speedup:<10.1f}x {break_even_calls:<12.0f}\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"- Compilation overhead decreases relative importance with larger arrays\")\n",
    "print(f\"- JIT is most beneficial for repeated calls on same-sized data\")\n",
    "print(f\"- Break-even point typically 2-10 calls depending on complexity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced_features",
   "metadata": {},
   "source": [
    "## Advanced JAX Features\n",
    "\n",
    "JAX provides additional transformations that can further improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced_demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad, jacfwd\n",
    "\n",
    "# Automatic differentiation example\n",
    "def complex_function(x):\n",
    "    \"\"\"Complex function for differentiation demo\"\"\"\n",
    "    return jnp.sum(x**3 - 2*x**2 + jnp.sin(x))\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = grad(complex_function)\n",
    "grad_fn_jit = jit(grad_fn)\n",
    "\n",
    "# Test on sample data\n",
    "x_test = jax.random.normal(key, (1000,))\n",
    "\n",
    "# Compare gradient computation times\n",
    "start = time.perf_counter()\n",
    "gradient = grad_fn(x_test)\n",
    "gradient.block_until_ready()\n",
    "grad_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "gradient_jit = grad_fn_jit(x_test)\n",
    "gradient_jit.block_until_ready()\n",
    "grad_jit_time = time.perf_counter() - start\n",
    "\n",
    "print(\"Automatic Differentiation Performance:\")\n",
    "print(f\"Gradient (no JIT): {grad_time:.6f}s\")\n",
    "print(f\"Gradient (JIT):    {grad_jit_time:.6f}s\")\n",
    "print(f\"AD + JIT speedup:  {grad_time/grad_jit_time:.1f}x\")\n",
    "\n",
    "# Demonstrate pmap for parallel computation (if multiple devices available)\n",
    "if len(jax.devices()) > 1:\n",
    "    from jax import pmap\n",
    "    print(f\"\\nMultiple devices available: {len(jax.devices())}\")\n",
    "    print(\"pmap can distribute computation across devices\")\n",
    "else:\n",
    "    print(f\"\\nSingle device setup - pmap would replicate across CPU cores\")\n",
    "\n",
    "print(f\"\\nJAX ecosystem advantages:\")\n",
    "print(f\"- Composable transformations: jit(vmap(grad(...)))\")\n",
    "print(f\"- Same code runs on CPU/GPU/TPU\")\n",
    "print(f\"- Functional programming paradigm\")\n",
    "print(f\"- Research-friendly with cutting-edge optimizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06f490-8f0a-4a5a-bd6c-cfe94b4cb5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd203fb0-cd32-4b34-ad2b-4239458dbb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa187f98-51e7-4cc7-8206-dfa3f1606f39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81846054-99b2-4201-8f76-4f8794a66da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
